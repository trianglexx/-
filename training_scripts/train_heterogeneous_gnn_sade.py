#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚æ„GNN + SADEæ··åˆæ¨¡å‹è®­ç»ƒè„šæœ¬
ç»“åˆå¼‚æ„å›¾ç¥ç»ç½‘ç»œçš„å¼ºå¤§ç‰¹å¾æå–èƒ½åŠ›ä¸SADEè‡ªé€‚åº”æŸå¤±å‡½æ•°
"""

import json
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
import logging
from pathlib import Path
from tqdm import tqdm
import copy
from typing import Dict, List, Optional

# PyG Imports
try:
    from torch_geometric.data import Dataset, Data
    from torch_geometric.loader import DataLoader
    from torch_geometric.nn import MessagePassing, global_mean_pool, global_max_pool
    from torch_geometric.utils import add_self_loops, degree
except ImportError:
    print("âŒ PyTorch Geometricä¸å¯ç”¨ï¼Œè¯·å®‰è£… pip install torch-geometric")
    exit(1)

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] INFO: %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)

class SADELoss(nn.Module):
    """
    SADE (Self-Adaptive Differential Evolution) æŸå¤±å‡½æ•°
    ç»“åˆäº†äº¤å‰ç†µæŸå¤±å’Œè‡ªé€‚åº”æƒé‡è°ƒæ•´ï¼Œä¸“é—¨ä¸ºå¼‚æ„GNNä¼˜åŒ–
    """
    
    def __init__(self, num_classes, alpha=1.0, beta=2.0, gamma=0.5):
        super(SADELoss, self).__init__()
        self.num_classes = num_classes
        self.alpha = alpha  # åŸºç¡€æƒé‡
        self.beta = beta    # ç±»åˆ«å¹³è¡¡æƒé‡
        self.gamma = gamma  # è‡ªé€‚åº”è°ƒæ•´æƒé‡
        
        # åˆå§‹åŒ–ç±»åˆ«æƒé‡
        self.register_buffer('class_weights', torch.ones(num_classes))
        self.register_buffer('class_counts', torch.zeros(num_classes))
        self.register_buffer('total_samples', torch.tensor(0.0))
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
        
    def update_class_statistics(self, targets):
        """æ›´æ–°ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯"""
        with torch.no_grad():
            for i in range(self.num_classes):
                count = (targets == i).sum().float()
                self.class_counts[i] += count
                self.total_samples += count
            
            # æ›´æ–°ç±»åˆ«æƒé‡ï¼ˆé€†é¢‘ç‡æƒé‡ï¼‰
            for i in range(self.num_classes):
                if self.class_counts[i] > 0:
                    freq = self.class_counts[i] / self.total_samples
                    self.class_weights[i] = 1.0 / (freq + 1e-8)
            
            # å½’ä¸€åŒ–æƒé‡
            self.class_weights = self.class_weights / self.class_weights.sum() * self.num_classes
    
    def compute_sade_weights(self, predictions, targets):
        """è®¡ç®—SADEè‡ªé€‚åº”æƒé‡"""
        batch_size = predictions.size(0)
        
        # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦
        probs = torch.softmax(predictions, dim=1)
        max_probs, pred_classes = torch.max(probs, dim=1)
        
        # è®¡ç®—é¢„æµ‹æ­£ç¡®æ€§
        correct_mask = (pred_classes == targets).float()
        
        # è‡ªé€‚åº”æƒé‡ï¼šé”™è¯¯é¢„æµ‹çš„æ ·æœ¬è·å¾—æ›´é«˜æƒé‡
        confidence_weights = 1.0 - max_probs
        correctness_weights = 1.0 - correct_mask
        
        # ç»“åˆç½®ä¿¡åº¦å’Œæ­£ç¡®æ€§
        sade_weights = 1.0 + self.gamma * (confidence_weights + correctness_weights)
        
        return sade_weights
    
    def forward(self, predictions, targets):
        """å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±"""
        # æ›´æ–°ç±»åˆ«ç»Ÿè®¡
        self.update_class_statistics(targets)
        
        # åŸºç¡€äº¤å‰ç†µæŸå¤±
        ce_losses = self.ce_loss(predictions, targets)
        
        # ç±»åˆ«æƒé‡
        class_weights_batch = self.class_weights[targets]
        
        # SADEè‡ªé€‚åº”æƒé‡
        sade_weights = self.compute_sade_weights(predictions, targets)
        
        # ç»„åˆæŸå¤±
        total_loss = self.alpha * ce_losses + self.beta * ce_losses * class_weights_batch + sade_weights * ce_losses
        
        return total_loss.mean()

class HeterogeneousGNNLayer(MessagePassing):
    """å¼‚æ„å›¾ç¥ç»ç½‘ç»œå±‚ - é’ˆå¯¹SADEæŸå¤±å‡½æ•°ä¼˜åŒ–"""
    
    def __init__(self,
                 input_dim: int,
                 output_dim: int,
                 num_edge_types: int = 4,
                 alpha: float = 0.1,
                 dropout: float = 0.2,
                 aggr: str = 'add'):
        super(HeterogeneousGNNLayer, self).__init__(aggr=aggr)
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_edge_types = num_edge_types
        self.alpha = alpha
        
        # ä¸ºæ¯ç§è¾¹ç±»å‹å®šä¹‰ä¸“ç”¨æƒé‡çŸ©é˜µ
        self.edge_type_weights = nn.ModuleList([
            nn.Linear(input_dim, output_dim, bias=False)
            for _ in range(num_edge_types)
        ])
        
        # è‡ªè¿æ¥æƒé‡çŸ©é˜µ
        self.self_weight = nn.Linear(input_dim, output_dim, bias=True)
        
        # è¾“å…¥æŠ•å½±ï¼ˆç”¨äºæ®‹å·®è¿æ¥ï¼‰
        if input_dim != output_dim:
            self.input_projection = nn.Linear(input_dim, output_dim, bias=False)
        else:
            self.input_projection = nn.Identity()
        
        # è¾¹ç±»å‹æ³¨æ„åŠ›æƒé‡
        self.edge_attention = nn.ModuleList([
            nn.Sequential(
                nn.Linear(output_dim * 2, output_dim // 2),
                nn.ReLU(),
                nn.Linear(output_dim // 2, 1),
                nn.LeakyReLU(0.2)
            ) for _ in range(num_edge_types)
        ])
        
        # æ­£åˆ™åŒ–
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(output_dim)
        
        # å±‚çº§åˆ«çš„è¾¹ç±»å‹é‡è¦æ€§æƒé‡
        self.layer_edge_importance = nn.Parameter(torch.ones(num_edge_types))
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """å‚æ•°åˆå§‹åŒ–"""
        gain = nn.init.calculate_gain('relu')
        
        for i in range(self.num_edge_types):
            nn.init.xavier_uniform_(self.edge_type_weights[i].weight, gain=gain)
        
        nn.init.xavier_uniform_(self.self_weight.weight, gain=gain)
        
        if hasattr(self.input_projection, 'weight'):
            nn.init.xavier_uniform_(self.input_projection.weight, gain=gain)
    
    def forward(self, x, edge_index_dict, batch_size=None):
        """å‰å‘ä¼ æ’­"""
        h_initial = x  # ä¿å­˜åˆå§‹ç‰¹å¾ç”¨äºæ®‹å·®è¿æ¥
        h_projected = self.input_projection(h_initial)
        
        # è‡ªè¿æ¥å˜æ¢
        h_self = self.self_weight(x)
        
        # åŠ¨æ€è¾¹æƒé‡è®¡ç®—
        edge_weights = F.softmax(self.layer_edge_importance, dim=0)
        
        # èšåˆä¸åŒè¾¹ç±»å‹çš„æ¶ˆæ¯
        h_neighbors = []
        
        for edge_type_idx, (edge_type, edge_index) in enumerate(edge_index_dict.items()):
            if edge_index.size(1) == 0:  # è·³è¿‡ç©ºè¾¹
                continue
                
            # ä½¿ç”¨å¯¹åº”è¾¹ç±»å‹çš„æƒé‡çŸ©é˜µ
            edge_weight = self.edge_type_weights[edge_type_idx]
            
            # æ¶ˆæ¯ä¼ é€’
            h_edge = self.propagate(
                edge_index, 
                x=edge_weight(x), 
                edge_type_idx=edge_type_idx,
                size=None
            )
            
            # åº”ç”¨è¾¹ç±»å‹é‡è¦æ€§æƒé‡
            h_edge = h_edge * edge_weights[edge_type_idx]
            h_neighbors.append(h_edge)
        
        # èšåˆæ‰€æœ‰è¾¹ç±»å‹çš„æ¶ˆæ¯
        if h_neighbors:
            h_agg = torch.stack(h_neighbors, dim=0).sum(dim=0)
        else:
            h_agg = torch.zeros_like(h_self)
        
        # ç»„åˆè‡ªè¿æ¥å’Œé‚»å±…èšåˆ
        h_out = (1 - self.alpha) * h_agg + h_self + self.alpha * h_projected
        
        # æ­£åˆ™åŒ–
        h_out = self.dropout(h_out)
        h_out = self.layer_norm(h_out)
        
        return h_out
    
    def message(self, x_j, edge_type_idx):
        """æ¶ˆæ¯å‡½æ•°"""
        return x_j

class HeterogeneousGNN(nn.Module):
    """å¼‚æ„å›¾ç¥ç»ç½‘ç»œæ¨¡å‹ - é›†æˆSADEæŸå¤±ä¼˜åŒ–"""
    
    def __init__(self, 
                 input_dim: int = 768,
                 hidden_dim: int = 256,
                 num_classes: int = 14,
                 num_layers: int = 3,
                 num_edge_types: int = 4,
                 dropout: float = 0.2,
                 alpha: float = 0.1):
        super(HeterogeneousGNN, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        self.num_layers = num_layers
        self.num_edge_types = num_edge_types
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # å¼‚æ„GNNå±‚
        self.gnn_layers = nn.ModuleList()
        for i in range(num_layers):
            self.gnn_layers.append(
                HeterogeneousGNNLayer(
                    input_dim=hidden_dim,
                    output_dim=hidden_dim,
                    num_edge_types=num_edge_types,
                    alpha=alpha,
                    dropout=dropout
                )
            )
        
        # åºåˆ—æŠ•å½±å±‚ï¼ˆå°†768ç»´æŠ•å½±åˆ°hidden_dimï¼‰
        self.sequence_projection = nn.Linear(input_dim, hidden_dim)
        
        # åºåˆ—å»ºæ¨¡å±‚ï¼ˆLSTMï¼‰
        self.sequence_lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim // 2,
            num_layers=2,
            batch_first=True,
            dropout=dropout,
            bidirectional=True
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Linear(hidden_dim // 2, num_classes)
        
        # å…¨å±€è¾¹ç±»å‹é‡è¦æ€§ï¼ˆè·¨å±‚å…±äº«ï¼‰
        self.global_edge_importance = nn.Parameter(torch.ones(num_edge_types))
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, data):
        """å‰å‘ä¼ æ’­"""
        x, edge_index_dict, batch = data.x, data.edge_index_dict, data.batch
        sequence_features = data.sequence_features
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ä¿å­˜åˆå§‹ç‰¹å¾
        x_initial = x
        
        # å¼‚æ„GNNå±‚
        for gnn_layer in self.gnn_layers:
            x_new = gnn_layer(x, edge_index_dict)
            x = x_new + x  # æ®‹å·®è¿æ¥
        
        # å›¾çº§åˆ«æ± åŒ–
        graph_features = global_mean_pool(x, batch)
        
        # åºåˆ—å»ºæ¨¡
        # ä½¿ç”¨åŸå§‹åºåˆ—ç‰¹å¾è€Œä¸æ˜¯æ„é€ è™šæ‹Ÿåºåˆ—
        batch_size = int(batch.max().item() + 1)
        
        # é‡æ–°ç»„ç»‡åºåˆ—ç‰¹å¾ä¸ºæ‰¹æ¬¡æ ¼å¼
        sequence_input_list = []
        for i in range(batch_size):
            # ä»batchä¸­æ‰¾åˆ°å¯¹åº”æ ·æœ¬çš„åºåˆ—ç‰¹å¾
            mask = (batch == i)
            sample_idx = mask.nonzero(as_tuple=True)[0][0]  # è·å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç´¢å¼•
            
            # è·å–å¯¹åº”æ ·æœ¬çš„åºåˆ—ç‰¹å¾
            seq_feat = sequence_features[sample_idx]  # [seq_len, 768]
            sequence_input_list.append(seq_feat)
        
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        max_seq_len = max(seq.size(0) for seq in sequence_input_list)
        padded_sequences = []
        
        for seq in sequence_input_list:
            if seq.size(0) < max_seq_len:
                padding = torch.zeros(max_seq_len - seq.size(0), seq.size(1)).to(seq.device)
                seq = torch.cat([seq, padding], dim=0)
            padded_sequences.append(seq)
        
        sequence_input = torch.stack(padded_sequences, dim=0)  # [batch_size, seq_len, 768]
        
        # æŠ•å½±åºåˆ—ç‰¹å¾åˆ°hidden_dim
        sequence_input = self.sequence_projection(sequence_input)  # [batch_size, seq_len, hidden_dim]
        
        # LSTMåºåˆ—å»ºæ¨¡
        lstm_out, (h_n, c_n) = self.sequence_lstm(sequence_input)
        
        # è°ƒè¯•ä¿¡æ¯
        # print(f"LSTM input shape: {sequence_input.shape}")
        # print(f"LSTM output shape: {lstm_out.shape}")
        
        # æ£€æŸ¥LSTMè¾“å‡ºå½¢çŠ¶å¹¶æ­£ç¡®å¤„ç†
        if lstm_out.dim() == 3:  # [batch_size, seq_len, hidden_dim]
            sequence_features_final = lstm_out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            
            # æ³¨æ„åŠ›æœºåˆ¶
            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
            attn_features = attn_out.mean(dim=1)  # å…¨å±€å¹³å‡
        else:  # å…¶ä»–æƒ…å†µå¤„ç†
            sequence_features_final = lstm_out.mean(dim=0) if lstm_out.dim() > 1 else lstm_out
            attn_features = sequence_features_final
        
        # ç¡®ä¿ç‰¹å¾ç»´åº¦åŒ¹é…
        if sequence_features_final.dim() == 1:
            sequence_features_final = sequence_features_final.unsqueeze(0)
        if graph_features.dim() != sequence_features_final.dim():
            if graph_features.dim() == 2 and sequence_features_final.dim() == 1:
                sequence_features_final = sequence_features_final.unsqueeze(0).expand(graph_features.size(0), -1)
            elif graph_features.dim() == 1 and sequence_features_final.dim() == 2:
                graph_features = graph_features.unsqueeze(0).expand(sequence_features_final.size(0), -1)
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([graph_features, sequence_features_final], dim=1)
        fused_features = self.fusion_layer(combined_features)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits
    
    def get_edge_importance_weights(self):
        """è·å–å­¦ä¹ åˆ°çš„è¾¹ç±»å‹é‡è¦æ€§æƒé‡"""
        with torch.no_grad():
            weights = F.softmax(self.global_edge_importance, dim=0).cpu().numpy()
        return weights.tolist()

class VulnerabilityDataset(Dataset):
    """æ¼æ´æ£€æµ‹æ•°æ®é›†"""
    
    def __init__(self, data_list):
        super(VulnerabilityDataset, self).__init__()
        self.data_list = data_list
    
    def len(self):
        return len(self.data_list)
    
    def get(self, idx):
        return self.data_list[idx]

def load_data(data_path: str):
    """åŠ è½½LIVABLEæ ¼å¼æ•°æ®å¹¶è½¬æ¢ä¸ºPyGæ ¼å¼"""
    logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
    
    with open(data_path, 'r') as f:
        raw_data = json.load(f)
    
    data_list = []
    edge_type_mapping = {'AST': 0, 'CFG': 1, 'DFG': 2, 'CDG': 3}
    
    for sample in tqdm(raw_data, desc="è½¬æ¢æ•°æ®æ ¼å¼"):
        # èŠ‚ç‚¹ç‰¹å¾ (LIVABLEæ ¼å¼ä¸­æ˜¯ 'features')
        # æ•°æ®æ ¼å¼æ˜¯ [seq_len, feature_dim]ï¼Œæˆ‘ä»¬å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºèŠ‚ç‚¹ç‰¹å¾
        features_seq = torch.tensor(sample['features'], dtype=torch.float32)
        
        # å¯¹äºå›¾ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬éœ€è¦å°†åºåˆ—æ•°æ®è½¬æ¢ä¸ºå•ä¸ªç‰¹å¾å‘é‡
        # æ–¹æ³•1: å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        node_features = features_seq[-1:, :]  # [1, 768]
        
        # æ–¹æ³•2: å–å¹³å‡ (å¯é€‰)
        # node_features = features_seq.mean(dim=0, keepdim=True)  # [1, 768]
        
        num_nodes = node_features.shape[0]
        
        # åˆå§‹åŒ–è¾¹ç´¢å¼•å­—å…¸
        edge_index_dict = {}
        for edge_type_name in edge_type_mapping.keys():
            edge_index_dict[edge_type_name] = torch.tensor([], dtype=torch.long).view(2, 0)
        
        # å¤„ç†è¾¹ (LIVABLEæ ¼å¼ä¸­æ˜¯ 'structure')
        if 'structure' in sample:
            for edge_info in sample['structure']:
                source, edge_type, target = edge_info
                
                # ç¡®ä¿è¾¹çš„èŠ‚ç‚¹ç´¢å¼•æœ‰æ•ˆ
                if source < num_nodes and target < num_nodes:
                    if edge_type in edge_type_mapping:
                        if edge_index_dict[edge_type].size(1) == 0:
                            edge_index_dict[edge_type] = torch.tensor([[source], [target]], dtype=torch.long)
                        else:
                            new_edge = torch.tensor([[source], [target]], dtype=torch.long)
                            edge_index_dict[edge_type] = torch.cat([edge_index_dict[edge_type], new_edge], dim=1)
        
        # æ ‡ç­¾
        label = torch.tensor(sample['label'], dtype=torch.long)
        
        # åˆ›å»ºDataå¯¹è±¡ï¼ŒåŒ…å«åºåˆ—ç‰¹å¾
        data = Data(
            x=node_features,  # å›¾èŠ‚ç‚¹ç‰¹å¾ [1, 768]
            edge_index_dict=edge_index_dict,
            sequence_features=features_seq,  # åºåˆ—ç‰¹å¾ [seq_len, 768]
            y=label
        )
        
        data_list.append(data)
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data_list)} ä¸ªæ ·æœ¬")
    return data_list

def train_model():
    """è®­ç»ƒæ··åˆæ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    train_data = load_data('livable_multiclass_data/livable_train.json')
    valid_data = load_data('livable_multiclass_data/livable_valid.json')
    test_data = load_data('livable_multiclass_data/livable_test.json')
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = VulnerabilityDataset(train_data)
    valid_dataset = VulnerabilityDataset(valid_data)
    test_dataset = VulnerabilityDataset(test_data)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = HeterogeneousGNN(
        input_dim=768,
        hidden_dim=256,
        num_classes=14,
        num_layers=3,
        num_edge_types=4,
        dropout=0.2,
        alpha=0.1
    ).to(device)
    
    # SADEæŸå¤±å‡½æ•°
    criterion = SADELoss(num_classes=14, alpha=1.0, beta=2.0, gamma=0.5).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=5)
    
    # è®­ç»ƒå‚æ•°
    num_epochs = 50
    best_valid_f1 = 0.0
    best_model = None
    patience = 10
    patience_counter = 0
    
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ··åˆæ¨¡å‹ (å¼‚æ„GNN + SADE)")
    
    training_history = {
        'train_loss': [],
        'train_f1': [],
        'valid_f1': []
    }
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            batch = batch.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch)
            loss = criterion(outputs, batch.y)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # è®°å½•é¢„æµ‹ç»“æœ
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch.y.cpu().numpy())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss = total_loss / len(train_loader)
        _, _, train_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        valid_preds = []
        valid_labels = []
        
        with torch.no_grad():
            for batch in valid_loader:
                batch = batch.to(device)
                outputs = model(batch)
                preds = torch.argmax(outputs, dim=1)
                valid_preds.extend(preds.cpu().numpy())
                valid_labels.extend(batch.y.cpu().numpy())
        
        _, _, valid_f1, _ = precision_recall_fscore_support(valid_labels, valid_preds, average='weighted')
        
        # è®°å½•å†å²
        training_history['train_loss'].append(train_loss)
        training_history['train_f1'].append(train_f1)
        training_history['valid_f1'].append(valid_f1)
        
        logger.info(f"Epoch {epoch+1}: Loss={train_loss:.4f}, Train F1={train_f1:.4f}, Valid F1={valid_f1:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if valid_f1 > best_valid_f1:
            best_valid_f1 = valid_f1
            best_model = copy.deepcopy(model)
            patience_counter = 0
            logger.info(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯F1: {best_valid_f1:.4f}")
        else:
            patience_counter += 1
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(valid_f1)
        
        # æ—©åœ
        if patience_counter >= patience:
            logger.info(f"â° æ—©åœè§¦å‘ï¼Œæœ€ä½³éªŒè¯F1: {best_valid_f1:.4f}")
            break
    
    # æµ‹è¯•é˜¶æ®µ
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•...")
    best_model.eval()
    test_preds = []
    test_labels = []
    
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            outputs = best_model(batch)
            preds = torch.argmax(outputs, dim=1)
            test_preds.extend(preds.cpu().numpy())
            test_labels.extend(batch.y.cpu().numpy())
    
    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
    test_accuracy = accuracy_score(test_labels, test_preds)
    test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')
    
    # è·å–è¾¹ç±»å‹é‡è¦æ€§
    edge_importance = best_model.get_edge_importance_weights()
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_type': 'Heterogeneous_GNN_SADE',
        'best_valid_f1': best_valid_f1,
        'test_accuracy': test_accuracy,
        'test_precision': test_precision, 
        'test_recall': test_recall,
        'test_f1': test_f1,
        'edge_type_importance': edge_importance,
        'model_parameters': sum(p.numel() for p in best_model.parameters()),
        'sade_parameters': {
            'alpha': 1.0,
            'beta': 2.0, 
            'gamma': 0.5
        },
        'classification_report': classification_report(test_labels, test_preds, target_names=[f'CWE-{i}' for i in [119, 20, 399, 125, 264, 200, 189, 416, 190, 362, 476, 787, 284, 254]]),
        'confusion_matrix': confusion_matrix(test_labels, test_preds).tolist()
    }
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = Path('heterogeneous_gnn_sade_results')
    results_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜ç»“æœ
    with open(results_dir / 'results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # ä¿å­˜æ¨¡å‹
    torch.save(best_model.state_dict(), results_dir / 'best_model.pth')
    
    # ä¿å­˜è®­ç»ƒå†å²
    with open(results_dir / 'training_history.json', 'w') as f:
        json.dump(training_history, f, indent=2)
    
    logger.info("ğŸ‰ æ··åˆæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    logger.info(f"   - å‡†ç¡®ç‡: {test_accuracy:.4f}")
    logger.info(f"   - F1åˆ†æ•°: {test_f1:.4f}")
    logger.info(f"   - éªŒè¯æœ€ä½³F1: {best_valid_f1:.4f}")
    logger.info(f"ğŸ”— è¾¹ç±»å‹é‡è¦æ€§:")
    edge_names = ['AST', 'CFG', 'DFG', 'CDG']
    for name, weight in zip(edge_names, edge_importance):
        logger.info(f"   - {name}: {weight:.4f}")

if __name__ == "__main__":
    train_model()