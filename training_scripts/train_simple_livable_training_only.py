#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆLIVABLEå¤šåˆ†ç±»è®­ç»ƒè„šæœ¬ - è®­ç»ƒé›†å­¦ä¹ åˆ†æç‰ˆæœ¬
åªåœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ 50è½®ï¼Œè§‚å¯Ÿæ¨¡å‹å­¦ä¹ æ•ˆæœ
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from datetime import datetime
import logging
from pathlib import Path
import torch.nn.functional as F
from tqdm import tqdm
import copy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class SimpleLIVABLEModel(nn.Module):
    """ç®€åŒ–ç‰ˆLIVABLEæ¨¡å‹ï¼ŒåŸºäºå›¾ç¥ç»ç½‘ç»œå’Œåºåˆ—æ¨¡å‹"""
    def __init__(self, input_dim=768, hidden_dim=256, num_classes=14, seq_input_dim=128):
        super(SimpleLIVABLEModel, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        self.seq_input_dim = seq_input_dim
        
        # å›¾åˆ†æ”¯ - ä½¿ç”¨ç®€å•çš„MLPå¤„ç†èŠ‚ç‚¹ç‰¹å¾
        self.graph_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # åºåˆ—åˆ†æ”¯ - ä½¿ç”¨GRUå¤„ç†åºåˆ—
        self.seq_encoder = nn.GRU(
            input_size=seq_input_dim,  # åºåˆ—ç‰¹å¾ç»´åº¦
            hidden_size=hidden_dim,
            num_layers=2,
            bidirectional=True,
            batch_first=True,
            dropout=0.2
        )
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim),  # å›¾ç‰¹å¾ + åŒå‘GRUç‰¹å¾
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, node_features, sequences):
        """å‰å‘ä¼ æ’­"""
        batch_size = node_features.size(0)
        
        # å›¾åˆ†æ”¯ï¼šå¤„ç†èŠ‚ç‚¹ç‰¹å¾
        # node_features: [batch_size, num_nodes, input_dim]
        graph_features = self.graph_encoder(node_features)  # [batch_size, num_nodes, hidden_dim]
        
        # å›¾çº§åˆ«æ± åŒ–
        graph_pooled = torch.mean(graph_features, dim=1)  # [batch_size, hidden_dim]
        
        # åºåˆ—åˆ†æ”¯ï¼šå¤„ç†åºåˆ—ç‰¹å¾
        # sequences: [batch_size, seq_len, seq_dim]
        seq_output, _ = self.seq_encoder(sequences)  # [batch_size, seq_len, hidden_dim*2]
        
        # åºåˆ—çº§åˆ«æ± åŒ–
        seq_pooled = torch.mean(seq_output, dim=1)  # [batch_size, hidden_dim*2]
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([graph_pooled, seq_pooled], dim=1)  # [batch_size, hidden_dim + hidden_dim*2]
        fused_features = self.fusion(combined)  # [batch_size, hidden_dim]
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)  # [batch_size, num_classes]
        
        return logits

class SimpleLIVABLEDataset(Dataset):
    """ç®€åŒ–ç‰ˆæ•°æ®é›†"""
    def __init__(self, data_path, max_nodes=100, max_seq_len=512):
        self.data = []
        self.max_nodes = max_nodes
        self.max_seq_len = max_seq_len
        
        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {data_path}")
        with open(data_path, 'r') as f:
            raw_data = json.load(f)
        
        logger.info(f"ğŸ“Š å¤„ç† {len(raw_data)} ä¸ªæ ·æœ¬...")
        for item in tqdm(raw_data, desc="å¤„ç†æ•°æ®"):
            try:
                features = item['features']  # èŠ‚ç‚¹ç‰¹å¾
                target = item['label'][0][0]  # æ ‡ç­¾
                sequence = item['sequence']  # åºåˆ—
                
                # å¤„ç†èŠ‚ç‚¹ç‰¹å¾ - å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šå¤§å°
                if len(features) > self.max_nodes:
                    features = features[:self.max_nodes]
                else:
                    # ç”¨é›¶å¡«å……
                    padding = [[0.0] * len(features[0])] * (self.max_nodes - len(features))
                    features = features + padding
                
                # å¤„ç†åºåˆ— - åºåˆ—å·²ç»æ˜¯ç‰¹å¾å‘é‡çš„åˆ—è¡¨
                seq_features = sequence  # ç›´æ¥ä½¿ç”¨åŸå§‹åºåˆ—ç‰¹å¾

                # ç¡®ä¿åºåˆ—ç‰¹å¾é•¿åº¦
                target_seq_len = 64  # ç›®æ ‡åºåˆ—é•¿åº¦
                if len(seq_features) > target_seq_len:
                    seq_features = seq_features[:target_seq_len]
                else:
                    # ç”¨é›¶å‘é‡å¡«å……
                    if len(seq_features) > 0:
                        feat_dim = len(seq_features[0])  # è·å–ç‰¹å¾ç»´åº¦
                        padding = [[0.0] * feat_dim] * (target_seq_len - len(seq_features))
                        seq_features = seq_features + padding
                    else:
                        # å¦‚æœåºåˆ—ä¸ºç©ºï¼Œåˆ›å»ºé›¶å‘é‡
                        seq_features = [[0.0] * 128] * target_seq_len
                
                self.data.append({
                    'features': torch.FloatTensor(features),
                    'sequence': torch.FloatTensor(seq_features),
                    'target': target
                })
                
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ ·æœ¬: {e}")
                continue
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} ä¸ªæ ·æœ¬")

        # è·å–åºåˆ—ç‰¹å¾ç»´åº¦
        if len(self.data) > 0:
            self.seq_feature_dim = len(self.data[0]['sequence'][0])
            logger.info(f"ğŸ“ åºåˆ—ç‰¹å¾ç»´åº¦: {self.seq_feature_dim}")
        else:
            self.seq_feature_dim = 128
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    features = torch.stack([item['features'] for item in batch])
    sequences = torch.stack([item['sequence'] for item in batch])
    targets = torch.LongTensor([item['target'] for item in batch])
    
    return features, sequences, targets

def train_simple_livable_training_analysis():
    """è®­ç»ƒç®€åŒ–ç‰ˆLIVABLE - ä¸“æ³¨è®­ç»ƒé›†å­¦ä¹ åˆ†æ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    logger.info("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = SimpleLIVABLEDataset('livable_multiclass_data/livable_train.json')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=16, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ—ï¸ åˆ›å»ºç®€åŒ–ç‰ˆLIVABLEæ¨¡å‹...")
    seq_feature_dim = train_dataset.seq_feature_dim
    model = SimpleLIVABLEModel(
        input_dim=768,
        hidden_dim=256,
        num_classes=14,
        seq_input_dim=seq_feature_dim
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    
    # è®­ç»ƒå‚æ•° - ä¸“æ³¨è®­ç»ƒé›†å­¦ä¹ åˆ†æ
    num_epochs = 50
    best_train_f1 = 0.0
    best_model = None
    
    logger.info("ğŸš€ å¼€å§‹ç®€åŒ–LIVABLEè®­ç»ƒé›†å­¦ä¹ åˆ†æ (50è½®)")
    
    training_history = {
        'train_loss': [],
        'train_accuracy': [],
        'train_f1': [],
        'class_accuracies_history': []
    }
    
    # CWEç±»åˆ«åç§°
    cwe_names = ['CWE-119', 'CWE-20', 'CWE-399', 'CWE-125', 'CWE-264', 'CWE-200', 
                 'CWE-189', 'CWE-416', 'CWE-190', 'CWE-362', 'CWE-476', 'CWE-787', 
                 'CWE-284', 'CWE-254']
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        for features, sequences, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            features = features.to(device)
            sequences = sequences.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            outputs = model(features, sequences)
            loss = criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            
            # è·å–é¢„æµ‹ç»“æœ
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(targets.cpu().numpy())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss = total_loss / len(train_loader)
        train_accuracy = accuracy_score(all_labels, all_preds)
        _, _, train_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for i in range(14):
            mask = np.array(all_labels) == i
            if mask.sum() > 0:
                class_acc = (np.array(all_preds)[mask] == i).sum() / mask.sum()
                class_accuracies[cwe_names[i]] = class_acc
            else:
                class_accuracies[cwe_names[i]] = 0.0
        
        # è®°å½•å†å²
        training_history['train_loss'].append(train_loss)
        training_history['train_accuracy'].append(train_accuracy)
        training_history['train_f1'].append(train_f1)
        training_history['class_accuracies_history'].append(class_accuracies.copy())
        
        logger.info(f"Epoch {epoch+1}: Loss={train_loss:.4f}, Train Acc={train_accuracy:.4f}, Train F1={train_f1:.4f}")
        
        # ä¿å­˜æœ€ä½³è®­ç»ƒæ¨¡å‹
        if train_f1 > best_train_f1:
            best_train_f1 = train_f1
            best_model = copy.deepcopy(model)
            logger.info(f"ğŸ¯ æ–°çš„æœ€ä½³è®­ç»ƒF1: {best_train_f1:.4f}")
    
    # æœ€ç»ˆè®­ç»ƒé›†åˆ†æ
    logger.info("ğŸ“Š åˆ†æè®­ç»ƒé›†å­¦ä¹ æ•ˆæœ...")
    
    # åˆ†æç±»åˆ«å‡†ç¡®ç‡å˜åŒ–
    logger.info("ğŸ¯ å„ç±»åˆ«è®­ç»ƒå‡†ç¡®ç‡å˜åŒ–åˆ†æ:")
    logger.info("=" * 80)
    
    first_epoch = training_history['class_accuracies_history'][0]
    last_epoch = training_history['class_accuracies_history'][-1]
    
    logger.info(f"{'CWEç±»åˆ«':<12} | {'åˆå§‹å‡†ç¡®ç‡':<12} | {'æœ€ç»ˆå‡†ç¡®ç‡':<12} | {'æå‡å¹…åº¦':<12} | {'ç›¸å¯¹æå‡':<12}")
    logger.info("-" * 80)
    
    for cwe in cwe_names:
        initial_acc = first_epoch[cwe]
        final_acc = last_epoch[cwe]
        improvement = final_acc - initial_acc
        relative_improvement = (improvement / (initial_acc + 1e-8)) * 100 if initial_acc > 0 else float('inf') if improvement > 0 else 0
        
        logger.info(f"{cwe:<12} | {initial_acc:>11.4f} | {final_acc:>11.4f} | {improvement:>+11.4f} | {relative_improvement:>+10.1f}%")
    
    # è®¡ç®—æœ€ç»ˆè®­ç»ƒé›†å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    logger.info("\nğŸ“‹ æœ€ç»ˆè®­ç»ƒé›†å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
    logger.info("=" * 80)
    
    # ä½¿ç”¨æœ€åä¸€è½®çš„é¢„æµ‹ç»“æœ
    final_precision, final_recall, final_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, zero_division=0)
    final_classification_report = classification_report(all_labels, all_preds, target_names=cwe_names, output_dict=True, zero_division=0)
    
    logger.info(f"{'CWEç±»åˆ«':<12} | {'å‡†ç¡®ç‡':<10} | {'ç²¾ç¡®ç‡':<10} | {'å¬å›ç‡':<10} | {'F1åˆ†æ•°':<10}")
    logger.info("-" * 70)
    
    for i, cwe in enumerate(cwe_names):
        accuracy = last_epoch[cwe]
        precision = final_precision[i] if i < len(final_precision) else 0.0
        recall = final_recall[i] if i < len(final_recall) else 0.0
        f1 = final_f1[i] if i < len(final_f1) else 0.0
        
        logger.info(f"{cwe:<12} | {accuracy:>9.4f} | {precision:>9.4f} | {recall:>9.4f} | {f1:>9.4f}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_type': 'Simple_LIVABLE_Training_Analysis',
        'final_train_f1': best_train_f1,
        'final_train_accuracy': training_history['train_accuracy'][-1],
        'final_train_loss': training_history['train_loss'][-1],
        'model_parameters': sum(p.numel() for p in best_model.parameters()),
        'training_history': training_history,
        'class_accuracy_analysis': {
            'initial': first_epoch,
            'final': last_epoch,
            'improvements': {cwe: last_epoch[cwe] - first_epoch[cwe] for cwe in cwe_names}
        },
        'final_classification_report': final_classification_report
    }
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = Path('simple_livable_training_analysis')
    results_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜ç»“æœ
    with open(results_dir / 'results.json', 'w') as f:
        json.dump(results, f, indent=2, default=float)
    
    torch.save(best_model.state_dict(), results_dir / 'best_model.pth')
    
    logger.info("ğŸ‰ ç®€åŒ–LIVABLEè®­ç»ƒåˆ†æå®Œæˆ!")
    logger.info(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒç»“æœ:")
    logger.info(f"   - æœ€ä½³è®­ç»ƒF1åˆ†æ•°: {best_train_f1:.4f}")
    logger.info(f"   - æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {training_history['train_accuracy'][-1]:.4f}")
    logger.info(f"   - æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_history['train_loss'][-1]:.4f}")
    
    # æ‰¾å‡ºæ”¹è¿›æœ€å¤§å’Œæœ€å°çš„ç±»åˆ«
    improvements = {cwe: last_epoch[cwe] - first_epoch[cwe] for cwe in cwe_names}
    best_improved = max(improvements, key=improvements.get)
    worst_improved = min(improvements, key=improvements.get)
    
    logger.info(f"ğŸ† æ”¹è¿›æœ€å¤§çš„ç±»åˆ«: {best_improved} (+{improvements[best_improved]:.4f})")
    logger.info(f"ğŸ“‰ æ”¹è¿›æœ€å°çš„ç±»åˆ«: {worst_improved} ({improvements[worst_improved]:+.4f})")

if __name__ == "__main__":
    train_simple_livable_training_analysis()