#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨SADEæ¨¡å‹ä½œä¸ºæŸå¤±å‡½æ•°è®­ç»ƒå¤šåˆ†ç±»LIVABLEæ¨¡å‹
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from datetime import datetime
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class SADELoss(nn.Module):
    """
    SADE (Self-Adaptive Differential Evolution) æŸå¤±å‡½æ•°
    ç»“åˆäº†äº¤å‰ç†µæŸå¤±å’Œè‡ªé€‚åº”æƒé‡è°ƒæ•´
    """
    
    def __init__(self, num_classes, alpha=1.0, beta=0.5, gamma=0.1):
        super(SADELoss, self).__init__()
        self.num_classes = num_classes
        self.alpha = alpha  # äº¤å‰ç†µæƒé‡
        self.beta = beta    # ç±»åˆ«å¹³è¡¡æƒé‡
        self.gamma = gamma  # è‡ªé€‚åº”è°ƒæ•´æƒé‡
        
        # åˆå§‹åŒ–ç±»åˆ«æƒé‡
        self.register_buffer('class_weights', torch.ones(num_classes))
        self.register_buffer('class_counts', torch.zeros(num_classes))
        self.register_buffer('total_samples', torch.tensor(0.0))
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
        
    def update_class_statistics(self, targets):
        """æ›´æ–°ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯"""
        with torch.no_grad():
            for i in range(self.num_classes):
                count = (targets == i).sum().float()
                self.class_counts[i] += count
                self.total_samples += count
            
            # æ›´æ–°ç±»åˆ«æƒé‡ï¼ˆé€†é¢‘ç‡æƒé‡ï¼‰
            for i in range(self.num_classes):
                if self.class_counts[i] > 0:
                    freq = self.class_counts[i] / self.total_samples
                    self.class_weights[i] = 1.0 / (freq + 1e-8)
            
            # å½’ä¸€åŒ–æƒé‡
            self.class_weights = self.class_weights / self.class_weights.sum() * self.num_classes
    
    def compute_sade_weights(self, predictions, targets):
        """è®¡ç®—SADEè‡ªé€‚åº”æƒé‡"""
        batch_size = predictions.size(0)
        
        # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦
        probs = torch.softmax(predictions, dim=1)
        max_probs, pred_classes = torch.max(probs, dim=1)
        
        # è®¡ç®—é¢„æµ‹æ­£ç¡®æ€§
        correct_mask = (pred_classes == targets).float()
        
        # è‡ªé€‚åº”æƒé‡ï¼šé”™è¯¯é¢„æµ‹çš„æ ·æœ¬è·å¾—æ›´é«˜æƒé‡
        confidence_weights = 1.0 - max_probs
        correctness_weights = 1.0 - correct_mask
        
        # ç»“åˆç½®ä¿¡åº¦å’Œæ­£ç¡®æ€§
        sade_weights = 1.0 + self.gamma * (confidence_weights + correctness_weights)
        
        return sade_weights
    
    def forward(self, predictions, targets):
        """å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±"""
        # æ›´æ–°ç±»åˆ«ç»Ÿè®¡
        self.update_class_statistics(targets)
        
        # åŸºç¡€äº¤å‰ç†µæŸå¤±
        ce_losses = self.ce_loss(predictions, targets)
        
        # ç±»åˆ«æƒé‡
        class_weights_batch = self.class_weights[targets]
        
        # SADEè‡ªé€‚åº”æƒé‡
        sade_weights = self.compute_sade_weights(predictions, targets)
        
        # ç»„åˆæŸå¤±
        weighted_losses = ce_losses * (
            self.alpha +  # åŸºç¡€æƒé‡
            self.beta * class_weights_batch +  # ç±»åˆ«å¹³è¡¡æƒé‡
            sade_weights  # SADEè‡ªé€‚åº”æƒé‡
        )
        
        return weighted_losses.mean()

class MulticlassLivableModelWithSADE(nn.Module):
    """å¸¦SADEæŸå¤±çš„å¤šåˆ†ç±»LIVABLEæ¨¡å‹"""
    
    def __init__(self, input_dim=768, hidden_dim=256, num_classes=14):
        super(MulticlassLivableModelWithSADE, self).__init__()
        
        # èŠ‚ç‚¹ç‰¹å¾å¤„ç†
        self.node_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # åºåˆ—ç‰¹å¾å¤„ç†
        self.sequence_encoder = nn.Sequential(
            nn.Linear(128, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # å›¾çº§æ± åŒ–
        self.graph_pooling = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # å¤šåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim // 4 + hidden_dim // 4, hidden_dim // 8),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim // 8, num_classes)
        )
    
    def forward(self, node_features, sequence_features):
        # å¤„ç†èŠ‚ç‚¹ç‰¹å¾
        node_encoded = self.node_encoder(node_features)
        
        # å›¾çº§æ± åŒ–
        graph_repr = torch.mean(node_encoded, dim=1)
        graph_repr = self.graph_pooling(graph_repr)
        
        # å¤„ç†åºåˆ—ç‰¹å¾
        seq_encoded = self.sequence_encoder(sequence_features)
        seq_repr = torch.mean(seq_encoded, dim=1)
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([graph_repr, seq_repr], dim=1)
        
        # å¤šåˆ†ç±»
        output = self.classifier(combined)
        
        return output

def setup_training_environment():
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("sade_training_results")
    output_dir.mkdir(exist_ok=True)
    
    return device, output_dir

def load_multiclass_data():
    """åŠ è½½å¤šåˆ†ç±»æ•°æ®"""
    
    logger.info("ğŸ“¥ åŠ è½½å¤šåˆ†ç±»è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½æ ‡ç­¾æ˜ å°„
    with open('multiclass_label_mapping.json', 'r') as f:
        mapping_info = json.load(f)
    
    logger.info(f"   ç±»åˆ«æ•°é‡: {mapping_info['num_classes']}")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {mapping_info['total_samples']}")
    
    # åŠ è½½æ•°æ®
    data_dir = Path("livable_multiclass_data")
    datasets = {}
    
    for split in ['train', 'valid', 'test']:
        file_path = data_dir / f"livable_{split}.json"
        
        logger.info(f"   åŠ è½½ {split} æ•°æ®: {file_path}")
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        datasets[split] = data
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        labels = [sample['label'][0][0] for sample in data]
        label_counts = {}
        for label in labels:
            label_counts[label] = label_counts.get(label, 0) + 1
        
        logger.info(f"   {split}: {len(data)} æ ·æœ¬, æ ‡ç­¾åˆ†å¸ƒ: {dict(sorted(label_counts.items()))}")
    
    return datasets, mapping_info

def prepare_batch_data(batch_samples, device, max_nodes=50):
    """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
    
    batch_size = len(batch_samples)
    
    # åˆå§‹åŒ–å¼ é‡
    node_features = torch.zeros(batch_size, max_nodes, 768, device=device)
    sequence_features = torch.zeros(batch_size, 6, 128, device=device)
    labels = torch.zeros(batch_size, dtype=torch.long, device=device)
    
    for i, sample in enumerate(batch_samples):
        # èŠ‚ç‚¹ç‰¹å¾
        features = sample['features']
        num_nodes = min(len(features), max_nodes)
        
        for j in range(num_nodes):
            node_features[i, j] = torch.tensor(features[j], dtype=torch.float)
        
        # åºåˆ—ç‰¹å¾
        sequence = sample['sequence']
        seq_len = min(len(sequence), 6)
        
        for j in range(seq_len):
            sequence_features[i, j] = torch.tensor(sequence[j], dtype=torch.float)
        
        # æ ‡ç­¾
        label = sample['label'][0][0] if sample['label'] else 0
        labels[i] = label
    
    return node_features, sequence_features, labels

def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    for batch_idx, batch_samples in enumerate(dataloader):
        optimizer.zero_grad()
        
        # å‡†å¤‡æ•°æ®
        node_features, sequence_features, labels = prepare_batch_data(batch_samples, device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(node_features, sequence_features)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
        if batch_idx % 50 == 0:
            logger.info(f"   æ‰¹æ¬¡ {batch_idx}/{len(dataloader)}, SADEæŸå¤±: {loss.item():.4f}")
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    
    return avg_loss, accuracy, all_preds, all_labels

def evaluate_model(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch_samples in dataloader:
            # å‡†å¤‡æ•°æ®
            node_features, sequence_features, labels = prepare_batch_data(batch_samples, device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(node_features, sequence_features)
            loss = criterion(outputs, labels)
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
    
    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    print("ğŸš€ ä½¿ç”¨SADEæŸå¤±å‡½æ•°è®­ç»ƒå¤šåˆ†ç±»LIVABLEæ¨¡å‹")
    print("Self-Adaptive Differential Evolution Loss")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    device, output_dir = setup_training_environment()
    
    # åŠ è½½æ•°æ®
    datasets, mapping_info = load_multiclass_data()
    num_classes = mapping_info['num_classes']
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 16
    
    def collate_fn(batch):
        return batch
    
    train_loader = DataLoader(datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    valid_loader = DataLoader(datasets['valid'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(datasets['test'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    logger.info(f"   éªŒè¯æ‰¹æ¬¡: {len(valid_loader)}")
    logger.info(f"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹å’ŒSADEæŸå¤±å‡½æ•°
    model = MulticlassLivableModelWithSADE(num_classes=num_classes).to(device)
    criterion = SADELoss(num_classes=num_classes, alpha=1.0, beta=2.0, gamma=0.5).to(device)
    
    logger.info(f"ğŸ§  æ¨¡å‹åˆ›å»ºå®Œæˆ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")
    logger.info(f"ğŸ¯ SADEæŸå¤±å‡½æ•°: alpha=1.0, beta=2.0, gamma=0.5")
    
    # è®¾ç½®ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    # è®­ç»ƒå¾ªç¯
    num_epochs = 40
    best_val_f1 = 0
    training_history = []
    
    logger.info(f"ğŸ¯ å¼€å§‹SADEè®­ç»ƒ {num_epochs} ä¸ªepoch...")
    
    for epoch in range(num_epochs):
        start_time = datetime.now()
        
        logger.info(f"\\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
        
        # è®­ç»ƒ
        train_loss, train_acc, train_preds, train_labels = train_epoch(
            model, train_loader, optimizer, criterion, device
        )
        
        # éªŒè¯
        val_loss, val_acc, val_precision, val_recall, val_f1, val_preds, val_labels = evaluate_model(
            model, valid_loader, criterion, device
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # è®°å½•å†å²
        epoch_info = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_precision': val_precision,
            'val_recall': val_recall,
            'val_f1': val_f1,
            'lr': optimizer.param_groups[0]['lr']
        }
        training_history.append(epoch_info)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), output_dir / "best_sade_model.pth")
            logger.info(f"   ğŸ’¾ ä¿å­˜æœ€ä½³SADEæ¨¡å‹ (F1: {val_f1:.4f})")
        
        # è¾“å‡ºç»“æœ
        duration = datetime.now() - start_time
        logger.info(f"   è®­ç»ƒSADEæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        logger.info(f"   éªŒè¯SADEæŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
        logger.info(f"   éªŒè¯F1: {val_f1:.4f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        logger.info(f"   è€—æ—¶: {duration}")
        
        # æ—©åœæ£€æŸ¥
        if epoch > 15 and val_f1 < best_val_f1 * 0.9:
            logger.info("   æ—©åœè§¦å‘")
            break
    
    # æœ€ç»ˆæµ‹è¯•
    logger.info("\\nğŸ¯ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    model.load_state_dict(torch.load(output_dir / "best_sade_model.pth"))
    test_loss, test_acc, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate_model(
        model, test_loader, criterion, device
    )
    
    logger.info(f"ğŸ“Š SADEè®­ç»ƒæœ€ç»ˆæµ‹è¯•ç»“æœ:")
    logger.info(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"   æµ‹è¯•ç²¾ç¡®ç‡: {test_precision:.4f}")
    logger.info(f"   æµ‹è¯•å¬å›ç‡: {test_recall:.4f}")
    logger.info(f"   æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    logger.info("\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    target_names = [f"{mapping_info['label_to_cwe'][str(i)]}" for i in range(num_classes)]
    report = classification_report(test_labels, test_preds, target_names=target_names)
    logger.info(f"\\n{report}")
    
    # ä¿å­˜ç»“æœ
    final_results = {
        'method': 'SADE_Loss',
        'best_val_f1': best_val_f1,
        'test_accuracy': test_acc,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'num_classes': num_classes,
        'total_samples': mapping_info['total_samples'],
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'sade_parameters': {
            'alpha': 1.0,
            'beta': 2.0,
            'gamma': 0.5
        },
        'classification_report': report,
        'label_mapping': mapping_info['label_to_cwe']
    }
    
    with open(output_dir / "sade_final_results.json", 'w') as f:
        json.dump(final_results, f, indent=2)
    
    with open(output_dir / "sade_training_history.json", 'w') as f:
        json.dump(training_history, f, indent=2)
    
    print("\\n" + "=" * 60)
    print("ğŸ‰ SADEè®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ† æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}")
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•F1: {test_f1:.4f}")
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    print("=" * 60)

if __name__ == "__main__":
    main()
