#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå¤šåˆ†ç±»LIVABLEæ¨¡å‹ï¼ˆ14ä¸ªCWEç±»åˆ«ï¼‰
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from datetime import datetime
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

def setup_training_environment():
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("multiclass_training_results")
    output_dir.mkdir(exist_ok=True)
    
    return device, output_dir

def load_multiclass_data():
    """åŠ è½½å¤šåˆ†ç±»æ•°æ®"""
    
    logger.info("ğŸ“¥ åŠ è½½å¤šåˆ†ç±»è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½æ ‡ç­¾æ˜ å°„
    with open('multiclass_label_mapping.json', 'r') as f:
        mapping_info = json.load(f)
    
    logger.info(f"   ç±»åˆ«æ•°é‡: {mapping_info['num_classes']}")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {mapping_info['total_samples']}")
    
    # åŠ è½½æ•°æ®
    data_dir = Path("livable_multiclass_data")
    datasets = {}
    
    for split in ['train', 'valid', 'test']:
        file_path = data_dir / f"livable_{split}.json"
        
        logger.info(f"   åŠ è½½ {split} æ•°æ®: {file_path}")
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        datasets[split] = data
        
        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        labels = [sample['label'][0][0] for sample in data]
        label_counts = {}
        for label in labels:
            label_counts[label] = label_counts.get(label, 0) + 1
        
        logger.info(f"   {split}: {len(data)} æ ·æœ¬, æ ‡ç­¾åˆ†å¸ƒ: {dict(sorted(label_counts.items()))}")
    
    return datasets, mapping_info

def create_multiclass_model(input_dim=768, hidden_dim=256, num_classes=14):
    """åˆ›å»ºå¤šåˆ†ç±»æ¨¡å‹"""
    
    class MulticlassLivableModel(nn.Module):
        def __init__(self, input_dim, hidden_dim, num_classes):
            super(MulticlassLivableModel, self).__init__()
            
            # èŠ‚ç‚¹ç‰¹å¾å¤„ç†
            self.node_encoder = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.2)
            )
            
            # åºåˆ—ç‰¹å¾å¤„ç†
            self.sequence_encoder = nn.Sequential(
                nn.Linear(128, hidden_dim // 4),
                nn.ReLU(),
                nn.Dropout(0.2)
            )
            
            # å›¾çº§æ± åŒ–
            self.graph_pooling = nn.Sequential(
                nn.Linear(hidden_dim // 2, hidden_dim // 4),
                nn.ReLU(),
                nn.Dropout(0.3)
            )
            
            # å¤šåˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.Linear(hidden_dim // 4 + hidden_dim // 4, hidden_dim // 8),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(hidden_dim // 8, num_classes)  # 14ä¸ªç±»åˆ«
            )
        
        def forward(self, node_features, sequence_features):
            # å¤„ç†èŠ‚ç‚¹ç‰¹å¾
            node_encoded = self.node_encoder(node_features)
            
            # å›¾çº§æ± åŒ–ï¼ˆå¹³å‡æ± åŒ–ï¼‰
            graph_repr = torch.mean(node_encoded, dim=1)
            graph_repr = self.graph_pooling(graph_repr)
            
            # å¤„ç†åºåˆ—ç‰¹å¾
            seq_encoded = self.sequence_encoder(sequence_features)
            seq_repr = torch.mean(seq_encoded, dim=1)
            
            # ç‰¹å¾èåˆ
            combined = torch.cat([graph_repr, seq_repr], dim=1)
            
            # å¤šåˆ†ç±»
            output = self.classifier(combined)
            
            return output
    
    return MulticlassLivableModel(input_dim, hidden_dim, num_classes)

def prepare_batch_data(batch_samples, device, max_nodes=50):
    """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
    
    batch_size = len(batch_samples)
    
    # åˆå§‹åŒ–å¼ é‡
    node_features = torch.zeros(batch_size, max_nodes, 768, device=device)
    sequence_features = torch.zeros(batch_size, 6, 128, device=device)
    labels = torch.zeros(batch_size, dtype=torch.long, device=device)
    
    for i, sample in enumerate(batch_samples):
        # èŠ‚ç‚¹ç‰¹å¾
        features = sample['features']
        num_nodes = min(len(features), max_nodes)
        
        for j in range(num_nodes):
            node_features[i, j] = torch.tensor(features[j], dtype=torch.float)
        
        # åºåˆ—ç‰¹å¾
        sequence = sample['sequence']
        seq_len = min(len(sequence), 6)
        
        for j in range(seq_len):
            sequence_features[i, j] = torch.tensor(sequence[j], dtype=torch.float)
        
        # æ ‡ç­¾
        label = sample['label'][0][0] if sample['label'] else 0
        labels[i] = label
    
    return node_features, sequence_features, labels

def train_epoch(model, dataloader, optimizer, criterion, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    for batch_idx, batch_samples in enumerate(dataloader):
        optimizer.zero_grad()
        
        # å‡†å¤‡æ•°æ®
        node_features, sequence_features, labels = prepare_batch_data(batch_samples, device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(node_features, sequence_features)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
        if batch_idx % 50 == 0:
            logger.info(f"   æ‰¹æ¬¡ {batch_idx}/{len(dataloader)}, æŸå¤±: {loss.item():.4f}")
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    
    return avg_loss, accuracy, all_preds, all_labels

def evaluate_model(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch_samples in dataloader:
            # å‡†å¤‡æ•°æ®
            node_features, sequence_features, labels = prepare_batch_data(batch_samples, device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(node_features, sequence_features)
            loss = criterion(outputs, labels)
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
    
    return avg_loss, accuracy, precision, recall, f1, all_preds, all_labels

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹å¤šåˆ†ç±»LIVABLEæ¨¡å‹è®­ç»ƒ")
    print("14ä¸ªCWEç±»åˆ«çš„æ¼æ´ç±»å‹åˆ†ç±»")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    device, output_dir = setup_training_environment()
    
    # åŠ è½½æ•°æ®
    datasets, mapping_info = load_multiclass_data()
    num_classes = mapping_info['num_classes']
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 16
    
    def collate_fn(batch):
        return batch
    
    train_loader = DataLoader(datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    valid_loader = DataLoader(datasets['valid'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    test_loader = DataLoader(datasets['test'], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    logger.info(f"   éªŒè¯æ‰¹æ¬¡: {len(valid_loader)}")
    logger.info(f"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_multiclass_model(num_classes=num_classes).to(device)
    logger.info(f"ğŸ§  æ¨¡å‹åˆ›å»ºå®Œæˆ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")
    
    # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)
    
    # è®­ç»ƒå¾ªç¯
    num_epochs = 30
    best_val_f1 = 0
    training_history = []
    
    logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepoch...")
    
    for epoch in range(num_epochs):
        start_time = datetime.now()
        
        logger.info(f"\\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
        
        # è®­ç»ƒ
        train_loss, train_acc, train_preds, train_labels = train_epoch(
            model, train_loader, optimizer, criterion, device
        )
        
        # éªŒè¯
        val_loss, val_acc, val_precision, val_recall, val_f1, val_preds, val_labels = evaluate_model(
            model, valid_loader, criterion, device
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # è®°å½•å†å²
        epoch_info = {
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'val_precision': val_precision,
            'val_recall': val_recall,
            'val_f1': val_f1,
            'lr': optimizer.param_groups[0]['lr']
        }
        training_history.append(epoch_info)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), output_dir / "best_multiclass_model.pth")
            logger.info(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1: {val_f1:.4f})")
        
        # è¾“å‡ºç»“æœ
        duration = datetime.now() - start_time
        logger.info(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        logger.info(f"   éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
        logger.info(f"   éªŒè¯F1: {val_f1:.4f}, å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        logger.info(f"   è€—æ—¶: {duration}")
        
        # æ—©åœæ£€æŸ¥
        if epoch > 10 and val_f1 < best_val_f1 * 0.95:
            logger.info("   æ—©åœè§¦å‘")
            break
    
    # æœ€ç»ˆæµ‹è¯•
    logger.info("\\nğŸ¯ æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")
    model.load_state_dict(torch.load(output_dir / "best_multiclass_model.pth"))
    test_loss, test_acc, test_precision, test_recall, test_f1, test_preds, test_labels = evaluate_model(
        model, test_loader, criterion, device
    )
    
    logger.info(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    logger.info(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"   æµ‹è¯•ç²¾ç¡®ç‡: {test_precision:.4f}")
    logger.info(f"   æµ‹è¯•å¬å›ç‡: {test_recall:.4f}")
    logger.info(f"   æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    logger.info("\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    target_names = [f"{mapping_info['label_to_cwe'][str(i)]}" for i in range(num_classes)]
    report = classification_report(test_labels, test_preds, target_names=target_names)
    logger.info(f"\\n{report}")
    
    # ä¿å­˜ç»“æœ
    final_results = {
        'best_val_f1': best_val_f1,
        'test_accuracy': test_acc,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'num_classes': num_classes,
        'total_samples': mapping_info['total_samples'],
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'classification_report': report,
        'label_mapping': mapping_info['label_to_cwe']
    }
    
    with open(output_dir / "final_results.json", 'w') as f:
        json.dump(final_results, f, indent=2)
    
    with open(output_dir / "training_history.json", 'w') as f:
        json.dump(training_history, f, indent=2)
    
    print("\\n" + "=" * 60)
    print("ğŸ‰ å¤šåˆ†ç±»è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ† æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}")
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•F1: {test_f1:.4f}")
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    print("=" * 60)

if __name__ == "__main__":
    main()
