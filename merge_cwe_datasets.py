#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆå¹¶CWEåˆ†ç±»æ•°æ®é›†
å°†æ¯ä¸ªCWEç±»å‹çš„train/valid/teståˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
import logging
from collections import Counter

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] INFO: %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)


class CWEDatasetMerger:
    """CWEæ•°æ®é›†åˆå¹¶å™¨"""
    
    def __init__(self, 
                 input_dir: str = "cwe_reveal_datasets", 
                 output_dir: str = "cwe_reveal_datasets_merged"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ”„ CWEæ•°æ®é›†åˆå¹¶å™¨åˆå§‹åŒ–")
        logger.info(f"ğŸ“‚ è¾“å…¥ç›®å½•: {self.input_dir}")
        logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def merge_cwe_splits(self, cwe_id: str) -> bool:
        """åˆå¹¶å•ä¸ªCWEç±»å‹çš„train/valid/testæ•°æ®"""
        logger.info(f"ğŸ”„ åˆå¹¶ {cwe_id} æ•°æ®é›†...")
        
        cwe_input_dir = self.input_dir / cwe_id.lower()
        cwe_output_dir = self.output_dir / cwe_id.lower()
        cwe_output_dir.mkdir(exist_ok=True)
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        splits = ["train", "valid", "test"]
        split_files = {}
        total_samples = 0
        
        for split in splits:
            split_file = cwe_input_dir / f"reveal-{split}-v2.json"
            if split_file.exists():
                split_files[split] = split_file
            else:
                logger.warning(f"âš ï¸ {cwe_id} {split} æ–‡ä»¶ä¸å­˜åœ¨: {split_file}")
        
        if not split_files:
            logger.error(f"âŒ {cwe_id} æ²¡æœ‰å¯åˆå¹¶çš„æ•°æ®æ–‡ä»¶")
            return False
        
        # åˆå¹¶æ•°æ®
        merged_data = []
        sample_id = 0
        split_counts = {}
        
        for split in splits:
            if split not in split_files:
                continue
                
            logger.info(f"  ğŸ“Š å¤„ç† {split} æ•°æ®...")
            
            with open(split_files[split], 'r', encoding='utf-8') as f:
                split_data = json.load(f)
            
            split_counts[split] = len(split_data)
            
            # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ æ¥æºä¿¡æ¯å¹¶é‡æ–°ç¼–å·
            for sample in split_data:
                # æ›´æ–°sample_id
                sample['metadata']['sample_id'] = sample_id
                sample['metadata']['original_split'] = split
                
                merged_data.append(sample)
                sample_id += 1
            
            logger.info(f"    âœ… {split}: {len(split_data)} æ ·æœ¬")
        
        total_samples = len(merged_data)
        logger.info(f"  ğŸ“ˆ åˆå¹¶å®Œæˆ: {total_samples} ä¸ªæ ·æœ¬")
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†
        output_file = cwe_output_dir / "complete_dataset.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"  ğŸ’¾ ä¿å­˜è‡³: {output_file}")
        
        # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯
        self.create_merged_stats(merged_data, cwe_id, cwe_output_dir, split_counts)
        
        # æ›´æ–°é…ç½®æ–‡ä»¶
        self.create_merged_config(cwe_id, cwe_output_dir, total_samples, split_counts)
        
        logger.info(f"ğŸ¯ {cwe_id} åˆå¹¶å®Œæˆ: {total_samples} ä¸ªæ ·æœ¬")
        return True
    
    def create_merged_stats(self, data: List[Dict], cwe_id: str, output_dir: Path, split_counts: Dict):
        """åˆ›å»ºåˆå¹¶æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "cwe_type": cwe_id,
            "total_samples": len(data),
            "original_split_distribution": split_counts,
            "feature_stats": {},
            "graph_stats": {},
            "metadata_analysis": {}
        }
        
        if data:
            # ç‰¹å¾ç»Ÿè®¡
            sample_features = [len(sample['node_features']) for sample in data]
            feature_dims = [len(sample['node_features'][0]) if sample['node_features'] else 0 for sample in data]
            
            stats["feature_stats"] = {
                "avg_nodes_per_sample": sum(sample_features) / len(sample_features),
                "max_nodes": max(sample_features),
                "min_nodes": min(sample_features),
                "feature_dim": max(feature_dims) if feature_dims else 0
            }
            
            # å›¾ç»“æ„ç»Ÿè®¡
            node_counts = [sample['metadata']['num_nodes'] for sample in data]
            edge_counts = [sample['metadata']['num_edges'] for sample in data]
            
            stats["graph_stats"] = {
                "avg_nodes": sum(node_counts) / len(node_counts),
                "avg_edges": sum(edge_counts) / len(edge_counts),
                "max_nodes": max(node_counts),
                "min_nodes": min(node_counts),
                "max_edges": max(edge_counts),
                "min_edges": min(edge_counts)
            }
            
            # å…ƒæ•°æ®åˆ†æ
            original_splits = [sample['metadata']['original_split'] for sample in data]
            split_distribution = dict(Counter(original_splits))
            
            stats["metadata_analysis"] = {
                "split_distribution": split_distribution,
                "binary_classification": data[0]['metadata'].get('binary_classification', True),
                "all_labels_are_1": all(sample['targets'] == [1] for sample in data)
            }
        
        # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶
        stats_file = output_dir / "dataset_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"    ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {stats_file}")
    
    def create_merged_config(self, cwe_id: str, output_dir: Path, total_samples: int, split_counts: Dict):
        """åˆ›å»ºåˆå¹¶æ•°æ®é›†çš„é…ç½®æ–‡ä»¶"""
        config = {
            "cwe_type": cwe_id,
            "format": "reveal_binary_classification_merged",
            "feature_dim": 768,
            "num_classes": 2,
            "class_labels": {
                "0": "normal",
                "1": "vulnerability"
            },
            "current_data": {
                "label": 1,
                "description": f"All samples are {cwe_id} vulnerabilities"
            },
            "graph_types": ["AST", "CFG", "DFG", "CDG"],
            "data_files": {
                "complete_dataset": "complete_dataset.json",
                "statistics": "dataset_statistics.json"
            },
            "total_samples": total_samples,
            "original_split_counts": split_counts,
            "merged_from": "cwe_reveal_datasets",
            "description": f"Merged dataset for {cwe_id} vulnerability type. Contains train/valid/test data combined into one file.",
            "usage": {
                "load_complete": "Load complete_dataset.json for all data",
                "filter_by_split": "Use 'original_split' field to filter by original train/valid/test",
                "custom_split": "Create your own train/valid/test splits from the complete dataset"
            },
            "note": "This is a merged dataset. Use 'original_split' metadata field to identify data source."
        }
        
        config_file = output_dir / "dataset_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"    ğŸ“‹ é…ç½®æ–‡ä»¶: {config_file}")
    
    def get_available_cwe_types(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„CWEç±»å‹"""
        cwe_types = []
        
        if not self.input_dir.exists():
            logger.error(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {self.input_dir}")
            return cwe_types
        
        for item in self.input_dir.iterdir():
            if item.is_dir() and item.name.startswith('cwe-'):
                cwe_id = item.name.upper().replace('CWE-', 'CWE-')
                cwe_types.append(cwe_id)
        
        return sorted(cwe_types)
    
    def merge_all_cwe_datasets(self) -> bool:
        """åˆå¹¶æ‰€æœ‰CWEæ•°æ®é›†"""
        logger.info("ğŸš€ å¼€å§‹åˆå¹¶æ‰€æœ‰CWEæ•°æ®é›†...")
        
        cwe_types = self.get_available_cwe_types()
        if not cwe_types:
            logger.error("âŒ æœªæ‰¾åˆ°å¯åˆå¹¶çš„CWEæ•°æ®é›†")
            return False
        
        logger.info(f"ğŸ“‹ å‘ç° {len(cwe_types)} ä¸ªCWEç±»å‹: {cwe_types}")
        
        success_count = 0
        failed_cwe = []
        
        for cwe_id in cwe_types:
            try:
                if self.merge_cwe_splits(cwe_id):
                    success_count += 1
                else:
                    failed_cwe.append(cwe_id)
            except Exception as e:
                logger.error(f"âŒ {cwe_id} åˆå¹¶å¤±è´¥: {e}")
                failed_cwe.append(cwe_id)
        
        # åˆ›å»ºæ€»ä½“è¯´æ˜æ–‡ä»¶
        self.create_master_readme(cwe_types, success_count, failed_cwe)
        
        if failed_cwe:
            logger.warning(f"âš ï¸ {len(failed_cwe)} ä¸ªCWEåˆå¹¶å¤±è´¥: {failed_cwe}")
        
        logger.info(f"ğŸ‰ åˆå¹¶å®Œæˆ! æˆåŠŸ: {success_count}/{len(cwe_types)}")
        return len(failed_cwe) == 0
    
    def create_master_readme(self, cwe_types: List[str], success_count: int, failed_cwe: List[str]):
        """åˆ›å»ºæ€»ä½“è¯´æ˜æ–‡ä»¶"""
        readme_content = f"""# CWEåˆ†ç±»åˆå¹¶æ•°æ®é›†

## ğŸ“Š æ•°æ®é›†æ¦‚è¿°

æœ¬æ•°æ®é›†æ˜¯CWEåˆ†ç±»Revealæ ¼å¼æ•°æ®é›†çš„åˆå¹¶ç‰ˆæœ¬ï¼Œå°†æ¯ä¸ªCWEç±»å‹çš„train/valid/testä¸‰ä¸ªæ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†ã€‚

## ğŸ¯ åˆå¹¶ä¼˜åŠ¿

- **æ•°æ®å®Œæ•´æ€§**: æ¯ä¸ªCWEç±»å‹çš„æ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
- **ä½¿ç”¨ä¾¿åˆ©**: æ— éœ€åˆ†åˆ«åŠ è½½å¤šä¸ªæ–‡ä»¶
- **çµæ´»åˆ†å‰²**: å¯æ ¹æ®éœ€è¦è‡ªå®šä¹‰train/valid/testæ¯”ä¾‹
- **æº¯æºæ¸…æ™°**: ä¿ç•™`original_split`å­—æ®µæ ‡è¯†æ•°æ®æ¥æº

## ğŸ“ ç›®å½•ç»“æ„

```
cwe_reveal_datasets_merged/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ merger_summary.json          # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
"""

        # æ·»åŠ æ¯ä¸ªCWEç±»å‹çš„ç›®å½•ç»“æ„
        for cwe_id in sorted(cwe_types):
            if cwe_id not in failed_cwe:
                readme_content += f"""â”œâ”€â”€ {cwe_id.lower()}/                    # {cwe_id} æ¼æ´ç±»å‹
â”‚   â”œâ”€â”€ complete_dataset.json       # å®Œæ•´æ•°æ®é›†
â”‚   â”œâ”€â”€ dataset_config.json         # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ dataset_statistics.json     # ç»Ÿè®¡ä¿¡æ¯
"""

        readme_content += f"""```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŠ è½½å®Œæ•´æ•°æ®é›†
```python
import json

# åŠ è½½CWE-119å®Œæ•´æ•°æ®é›†
with open('cwe_reveal_datasets_merged/cwe-119/complete_dataset.json', 'r') as f:
    cwe119_data = json.load(f)

print(f"CWE-119æ€»æ ·æœ¬æ•°: {{len(cwe119_data)}}")
print(f"æ ·æœ¬æ ¼å¼: {{list(cwe119_data[0].keys())}}")
```

### æŒ‰åŸå§‹åˆ†å‰²è¿‡æ»¤æ•°æ®
```python
# æŒ‰åŸå§‹train/valid/teståˆ†å‰²è¿‡æ»¤
train_samples = [s for s in cwe119_data if s['metadata']['original_split'] == 'train']
valid_samples = [s for s in cwe119_data if s['metadata']['original_split'] == 'valid'] 
test_samples = [s for s in cwe119_data if s['metadata']['original_split'] == 'test']

print(f"è®­ç»ƒ: {{len(train_samples)}}, éªŒè¯: {{len(valid_samples)}}, æµ‹è¯•: {{len(test_samples)}}")
```

### è‡ªå®šä¹‰æ•°æ®åˆ†å‰²
```python
from sklearn.model_selection import train_test_split

# è‡ªå®šä¹‰70/15/15åˆ†å‰²
train_data, temp_data = train_test_split(cwe119_data, test_size=0.3, random_state=42)
valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

print(f"è‡ªå®šä¹‰åˆ†å‰² - è®­ç»ƒ: {{len(train_data)}}, éªŒè¯: {{len(valid_data)}}, æµ‹è¯•: {{len(test_data)}}")
```

## ğŸ“Š åˆå¹¶ç»Ÿè®¡

- **æˆåŠŸåˆå¹¶**: {success_count}/{len(cwe_types)} ä¸ªCWEç±»å‹
- **æ•°æ®æ ¼å¼**: æ ‡å‡†Revealæ ¼å¼ + åˆå¹¶å…ƒæ•°æ®
- **ç‰¹å¾ç»´åº¦**: 768ç»´GraphCodeBERTåµŒå…¥
- **æ ‡ç­¾ç±»å‹**: äºŒåˆ†ç±»ï¼ˆæ‰€æœ‰æ ·æœ¬æ ‡ç­¾=1ï¼Œè¡¨ç¤ºæ¼æ´ï¼‰

"""

        if failed_cwe:
            readme_content += f"""
## âš ï¸ åˆå¹¶å¤±è´¥çš„CWEç±»å‹

ä»¥ä¸‹CWEç±»å‹åˆå¹¶å¤±è´¥:
"""
            for cwe_id in failed_cwe:
                readme_content += f"- {cwe_id}\n"

        readme_content += f"""
## ğŸ“ˆ æ•°æ®ç‰¹ç‚¹

- **å®Œæ•´æ€§**: æ¯ä¸ªCWEç±»å‹åŒ…å«æ‰€æœ‰è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®
- **æº¯æºæ€§**: `original_split`å­—æ®µæ ‡è¯†æ•°æ®æ¥æº
- **ä¸€è‡´æ€§**: æ‰€æœ‰æ ·æœ¬ä¿æŒRevealæ ‡å‡†æ ¼å¼
- **äºŒåˆ†ç±»**: æ ‡ç­¾å‡ä¸º1ï¼ˆæ¼æ´ï¼‰ï¼Œéœ€æ·»åŠ æ­£å¸¸æ ·æœ¬å®ŒæˆäºŒåˆ†ç±»

## ğŸ¯ åº”ç”¨åœºæ™¯

1. **å®Œæ•´CWEåˆ†æ**: å¯¹ç‰¹å®šæ¼æ´ç±»å‹è¿›è¡Œå…¨é¢åˆ†æ
2. **è‡ªå®šä¹‰æ•°æ®åˆ†å‰²**: æ ¹æ®ç ”ç©¶éœ€æ±‚é‡æ–°åˆ†å‰²æ•°æ®
3. **è·¨CWEå¯¹æ¯”**: æ¯”è¾ƒä¸åŒæ¼æ´ç±»å‹çš„ç‰¹å¾å·®å¼‚
4. **æ¨¡å‹è®­ç»ƒ**: ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒæ›´ç¨³å®šçš„æ¨¡å‹

## ğŸ’¡ ä½¿ç”¨æç¤º

1. **å†…å­˜å ç”¨**: å¤§å‹CWEç±»å‹ï¼ˆå¦‚CWE-119ï¼‰æ•°æ®è¾ƒå¤§ï¼Œæ³¨æ„å†…å­˜ä½¿ç”¨
2. **æ•°æ®å®Œæ•´æ€§**: ä½¿ç”¨å‰å¯é€šè¿‡ç»Ÿè®¡æ–‡ä»¶éªŒè¯æ•°æ®å®Œæ•´æ€§
3. **æ ‡ç­¾ä¸€è‡´æ€§**: æ‰€æœ‰æ ·æœ¬æ ‡ç­¾ä¸º1ï¼Œéœ€è¦æ·»åŠ æ­£å¸¸æ ·æœ¬è¿›è¡ŒäºŒåˆ†ç±»
4. **åˆ†å‰²çµæ´»æ€§**: å¯æ ¹æ®ç ”ç©¶éœ€æ±‚è‡ªå®šä¹‰train/valid/testæ¯”ä¾‹

ç”Ÿæˆæ—¶é—´: {self.get_current_time()}
"""

        # ä¿å­˜README
        readme_file = self.output_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        # åˆ›å»ºåˆå¹¶æ‘˜è¦
        summary = {
            "total_cwe_types": len(cwe_types),
            "successfully_merged": success_count,
            "failed_cwe_types": failed_cwe,
            "merge_strategy": "train_valid_test_combined",
            "output_format": "single_complete_dataset_per_cwe",
            "preserved_fields": [
                "node_features", "graph", "targets", "metadata"
            ],
            "added_fields": [
                "original_split (in metadata)"
            ],
            "data_features": {
                "feature_dim": 768,
                "format": "reveal_binary_classification_merged",
                "all_labels": 1,
                "graph_types": ["AST", "CFG", "DFG", "CDG"]
            }
        }
        
        summary_file = self.output_dir / "merger_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ æ€»ä½“è¯´æ˜: {readme_file}")
        logger.info(f"ğŸ“Š åˆå¹¶æ‘˜è¦: {summary_file}")
    
    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        import datetime
        return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def main():
    """ä¸»å‡½æ•°"""
    merger = CWEDatasetMerger()
    
    success = merger.merge_all_cwe_datasets()
    
    if success:
        print(f"\nğŸ‰ æ‰€æœ‰CWEæ•°æ®é›†åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: cwe_reveal_datasets_merged/")
        print(f"\nğŸ“Š åˆå¹¶ç‰¹ç‚¹:")
        print(f"  âœ… æ¯ä¸ªCWEç±»å‹åˆå¹¶ä¸ºå•ä¸€å®Œæ•´æ•°æ®é›†")
        print(f"  âœ… ä¿ç•™original_splitå­—æ®µæ ‡è¯†æ•°æ®æ¥æº")
        print(f"  âœ… é‡æ–°ç¼–å·sample_idä¿è¯å”¯ä¸€æ€§")
        print(f"  âœ… ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡å’Œé…ç½®ä¿¡æ¯")
        print(f"  âœ… æ ‡å‡†Revealæ ¼å¼ + 768ç»´ç‰¹å¾")
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"  - æŸ¥çœ‹ cwe_reveal_datasets_merged/README.md")
        print(f"  - ä½¿ç”¨complete_dataset.jsonåŠ è½½å®Œæ•´æ•°æ®")
        print(f"  - é€šè¿‡original_splitå­—æ®µè¿‡æ»¤æ•°æ®")
        print(f"  - å¯è‡ªå®šä¹‰train/valid/teståˆ†å‰²æ¯”ä¾‹")
        return 0
    else:
        print(f"\nâŒ éƒ¨åˆ†CWEæ•°æ®é›†åˆå¹¶å¤±è´¥")
        print(f"è¯·æ£€æŸ¥æ—¥å¿—äº†è§£è¯¦æƒ…")
        return 1


if __name__ == "__main__":
    exit(main())