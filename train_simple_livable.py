#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆLIVABLEå¤šåˆ†ç±»è®­ç»ƒè„šæœ¬
ä½¿ç”¨PyTorchåŸç”ŸåŠŸèƒ½ï¼Œé¿å…DGLå…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from datetime import datetime
import logging
from pathlib import Path
import torch.nn.functional as F
from tqdm import tqdm
import copy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class SimpleLIVABLEModel(nn.Module):
    """ç®€åŒ–ç‰ˆLIVABLEæ¨¡å‹ï¼ŒåŸºäºå›¾ç¥ç»ç½‘ç»œå’Œåºåˆ—æ¨¡å‹"""
    def __init__(self, input_dim=768, hidden_dim=256, num_classes=14, seq_input_dim=128):
        super(SimpleLIVABLEModel, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        self.seq_input_dim = seq_input_dim
        
        # å›¾åˆ†æ”¯ - ä½¿ç”¨ç®€å•çš„MLPå¤„ç†èŠ‚ç‚¹ç‰¹å¾
        self.graph_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # åºåˆ—åˆ†æ”¯ - ä½¿ç”¨GRUå¤„ç†åºåˆ—
        self.seq_encoder = nn.GRU(
            input_size=seq_input_dim,  # åºåˆ—ç‰¹å¾ç»´åº¦
            hidden_size=hidden_dim,
            num_layers=2,
            bidirectional=True,
            batch_first=True,
            dropout=0.2
        )
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim),  # å›¾ç‰¹å¾ + åŒå‘GRUç‰¹å¾
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, node_features, sequences):
        """å‰å‘ä¼ æ’­"""
        batch_size = node_features.size(0)
        
        # å›¾åˆ†æ”¯ï¼šå¤„ç†èŠ‚ç‚¹ç‰¹å¾
        # node_features: [batch_size, num_nodes, input_dim]
        graph_features = self.graph_encoder(node_features)  # [batch_size, num_nodes, hidden_dim]
        
        # å›¾çº§åˆ«æ± åŒ–
        graph_pooled = torch.mean(graph_features, dim=1)  # [batch_size, hidden_dim]
        
        # åºåˆ—åˆ†æ”¯ï¼šå¤„ç†åºåˆ—ç‰¹å¾
        # sequences: [batch_size, seq_len, seq_dim]
        seq_output, _ = self.seq_encoder(sequences)  # [batch_size, seq_len, hidden_dim*2]
        
        # åºåˆ—çº§åˆ«æ± åŒ–
        seq_pooled = torch.mean(seq_output, dim=1)  # [batch_size, hidden_dim*2]
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([graph_pooled, seq_pooled], dim=1)  # [batch_size, hidden_dim + hidden_dim*2]
        fused_features = self.fusion(combined)  # [batch_size, hidden_dim]
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)  # [batch_size, num_classes]
        
        return logits

class SimpleLIVABLEDataset(Dataset):
    """ç®€åŒ–ç‰ˆæ•°æ®é›†"""
    def __init__(self, data_path, max_nodes=100, max_seq_len=512):
        self.data = []
        self.max_nodes = max_nodes
        self.max_seq_len = max_seq_len
        
        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {data_path}")
        with open(data_path, 'r') as f:
            raw_data = json.load(f)
        
        logger.info(f"ğŸ“Š å¤„ç† {len(raw_data)} ä¸ªæ ·æœ¬...")
        for item in tqdm(raw_data, desc="å¤„ç†æ•°æ®"):
            try:
                features = item['features']  # èŠ‚ç‚¹ç‰¹å¾
                target = item['label'][0][0]  # æ ‡ç­¾
                sequence = item['sequence']  # åºåˆ—
                
                # å¤„ç†èŠ‚ç‚¹ç‰¹å¾ - å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šå¤§å°
                if len(features) > self.max_nodes:
                    features = features[:self.max_nodes]
                else:
                    # ç”¨é›¶å¡«å……
                    padding = [[0.0] * len(features[0])] * (self.max_nodes - len(features))
                    features = features + padding
                
                # å¤„ç†åºåˆ— - åºåˆ—å·²ç»æ˜¯ç‰¹å¾å‘é‡çš„åˆ—è¡¨
                seq_features = sequence  # ç›´æ¥ä½¿ç”¨åŸå§‹åºåˆ—ç‰¹å¾

                # ç¡®ä¿åºåˆ—ç‰¹å¾é•¿åº¦
                target_seq_len = 64  # ç›®æ ‡åºåˆ—é•¿åº¦
                if len(seq_features) > target_seq_len:
                    seq_features = seq_features[:target_seq_len]
                else:
                    # ç”¨é›¶å‘é‡å¡«å……
                    if len(seq_features) > 0:
                        feat_dim = len(seq_features[0])  # è·å–ç‰¹å¾ç»´åº¦
                        padding = [[0.0] * feat_dim] * (target_seq_len - len(seq_features))
                        seq_features = seq_features + padding
                    else:
                        # å¦‚æœåºåˆ—ä¸ºç©ºï¼Œåˆ›å»ºé›¶å‘é‡
                        seq_features = [[0.0] * 128] * target_seq_len
                
                self.data.append({
                    'features': torch.FloatTensor(features),
                    'sequence': torch.FloatTensor(seq_features),
                    'target': target
                })
                
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ ·æœ¬: {e}")
                continue
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} ä¸ªæ ·æœ¬")

        # è·å–åºåˆ—ç‰¹å¾ç»´åº¦
        if len(self.data) > 0:
            self.seq_feature_dim = len(self.data[0]['sequence'][0])
            logger.info(f"ğŸ“ åºåˆ—ç‰¹å¾ç»´åº¦: {self.seq_feature_dim}")
        else:
            self.seq_feature_dim = 128
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    features = torch.stack([item['features'] for item in batch])
    sequences = torch.stack([item['sequence'] for item in batch])
    targets = torch.LongTensor([item['target'] for item in batch])
    
    return features, sequences, targets

def setup_training_environment():
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("simple_livable_results")
    output_dir.mkdir(exist_ok=True)
    
    return device, output_dir

def load_datasets():
    """åŠ è½½è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†"""
    train_dataset = SimpleLIVABLEDataset('livable_multiclass_data/livable_train.json')
    valid_dataset = SimpleLIVABLEDataset('livable_multiclass_data/livable_valid.json')
    test_dataset = SimpleLIVABLEDataset('livable_multiclass_data/livable_test.json')
    
    return train_dataset, valid_dataset, test_dataset

def create_data_loaders(train_dataset, valid_dataset, test_dataset, batch_size=16):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    valid_loader = DataLoader(
        valid_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    return train_loader, valid_loader, test_loader

def evaluate_model(model, data_loader, device, criterion):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for features, sequences, targets in tqdm(data_loader, desc="è¯„ä¼°ä¸­"):
            features = features.to(device)
            sequences = sequences.to(device)
            targets = targets.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(features, sequences)
            loss = criterion(outputs, targets)

            total_loss += loss.item()

            # è·å–é¢„æµ‹ç»“æœ
            _, predicted = torch.max(outputs.data, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted', zero_division=0
    )

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracies = {}
    conf_matrix = confusion_matrix(all_targets, all_predictions)
    for i in range(len(conf_matrix)):
        if conf_matrix[i].sum() > 0:  # é¿å…é™¤é›¶
            class_accuracies[i] = conf_matrix[i][i] / conf_matrix[i].sum()
        else:
            class_accuracies[i] = 0.0

    avg_loss = total_loss / len(data_loader)

    return avg_loss, accuracy, precision, recall, f1, all_predictions, all_targets, class_accuracies

def train_model(model, train_loader, valid_loader, device, num_epochs=100, learning_rate=0.001):
    """è®­ç»ƒæ¨¡å‹ - ä¸ä½¿ç”¨æ—©åœï¼Œè®­ç»ƒæ»¡100ä¸ªepoch"""
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

    best_valid_f1 = 0
    best_model_state = None

    training_history = {
        'train_loss': [],
        'train_acc': [],
        'valid_loss': [],
        'valid_acc': [],
        'valid_f1': []
    }

    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepochï¼ˆä¸ä½¿ç”¨æ—©åœï¼‰")

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for features, sequences, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            features = features.to(device)
            sequences = sequences.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(features, sequences)
            loss = criterion(outputs, targets)

            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += targets.size(0)
            train_correct += (predicted == targets).sum().item()

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_accuracy = train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)

        # éªŒè¯é˜¶æ®µ
        valid_loss, valid_accuracy, valid_precision, valid_recall, valid_f1, _, _, _ = evaluate_model(
            model, valid_loader, device, criterion
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(valid_f1)

        # è®°å½•å†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['train_acc'].append(train_accuracy)
        training_history['valid_loss'].append(valid_loss)
        training_history['valid_acc'].append(valid_accuracy)
        training_history['valid_f1'].append(valid_f1)

        logger.info(f"Epoch {epoch+1}/{num_epochs}:")
        logger.info(f"  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}")
        logger.info(f"  éªŒè¯ - Loss: {valid_loss:.4f}, Acc: {valid_accuracy:.4f}, F1: {valid_f1:.4f}")
        logger.info(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä½†ä¸æ—©åœï¼‰
        if valid_f1 > best_valid_f1:
            best_valid_f1 = valid_f1
            best_model_state = copy.deepcopy(model.state_dict())
            logger.info(f"  âœ… æ–°çš„æœ€ä½³F1åˆ†æ•°: {best_valid_f1:.4f}")

    # æ¢å¤æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    logger.info(f"ğŸ¯ è®­ç»ƒå®Œæˆï¼è®­ç»ƒäº†å®Œæ•´çš„ {num_epochs} ä¸ªepochï¼Œæœ€ä½³éªŒè¯F1: {best_valid_f1:.4f}")

    return model, training_history, best_valid_f1

def save_results(model, training_history, test_results, output_dir):
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / "best_simple_livable_model.pth"
    torch.save(model.state_dict(), model_path)
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = output_dir / "simple_livable_training_history.json"
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2)
    logger.info(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

    # ä¿å­˜æµ‹è¯•ç»“æœ
    results_path = output_dir / "simple_livable_final_results.json"
    with open(results_path, 'w') as f:
        json.dump(test_results, f, indent=2)
    logger.info(f"ğŸ“ˆ æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¼€å§‹ç®€åŒ–ç‰ˆLIVABLEå¤šåˆ†ç±»æ¼æ´æ£€æµ‹è®­ç»ƒ")

    # è®¾ç½®ç¯å¢ƒ
    device, output_dir = setup_training_environment()

    # åŠ è½½æ•°æ®
    logger.info("ğŸ“¥ åŠ è½½æ•°æ®é›†...")
    train_dataset, valid_dataset, test_dataset = load_datasets()

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, valid_loader, test_loader = create_data_loaders(
        train_dataset, valid_dataset, test_dataset, batch_size=16
    )

    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ—ï¸ åˆ›å»ºç®€åŒ–ç‰ˆLIVABLEæ¨¡å‹...")
    seq_feature_dim = train_dataset.seq_feature_dim
    model = SimpleLIVABLEModel(
        input_dim=768,
        hidden_dim=256,
        num_classes=14,
        seq_input_dim=seq_feature_dim
    ).to(device)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")

    # è®­ç»ƒæ¨¡å‹ï¼ˆä¸ä½¿ç”¨æ—©åœï¼Œè®­ç»ƒæ»¡100ä¸ªepochï¼‰
    model, training_history, best_valid_f1 = train_model(
        model, train_loader, valid_loader, device, num_epochs=100, learning_rate=0.001
    )

    # æµ‹è¯•æ¨¡å‹
    logger.info("ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    criterion = nn.CrossEntropyLoss()
    test_loss, test_accuracy, test_precision, test_recall, test_f1, predictions, targets, class_accuracies = evaluate_model(
        model, test_loader, device, criterion
    )

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    with open('multiclass_label_mapping.json', 'r') as f:
        label_mapping = json.load(f)

    class_names = [label_mapping['label_to_cwe'][str(i)] for i in range(14)]
    classification_rep = classification_report(
        targets, predictions, target_names=class_names, output_dict=True, zero_division=0
    )

    # æ··æ·†çŸ©é˜µ
    conf_matrix = confusion_matrix(targets, predictions)

    # æ•´ç†æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracy_dict = {}
    for i, class_name in enumerate(class_names):
        class_accuracy_dict[class_name] = class_accuracies.get(i, 0.0)

    # æ•´ç†æµ‹è¯•ç»“æœ
    test_results = {
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'best_valid_f1': best_valid_f1,
        'classification_report': classification_rep,
        'confusion_matrix': conf_matrix.tolist(),
        'class_names': class_names,
        'class_accuracies': class_accuracy_dict,
        'model_info': {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'input_dim': 768,
            'hidden_dim': 256,
            'num_classes': 14,
            'epochs_trained': 100,
            'early_stopping': False
        }
    }

    # æ‰“å°ç»“æœ
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    logger.info(f"  å‡†ç¡®ç‡: {test_accuracy:.4f}")
    logger.info(f"  ç²¾ç¡®ç‡: {test_precision:.4f}")
    logger.info(f"  å¬å›ç‡: {test_recall:.4f}")
    logger.info(f"  F1åˆ†æ•°: {test_f1:.4f}")

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    logger.info("ğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡:")
    for class_name, acc in class_accuracy_dict.items():
        logger.info(f"  {class_name}: {acc:.4f}")

    # ä¿å­˜ç»“æœ
    save_results(model, training_history, test_results, output_dir)

    logger.info("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜å®Œæˆï¼")

    # ä¿æŒç»ˆç«¯å¼€å¯ï¼Œæ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    logger.info("ğŸ”„ è®­ç»ƒå®Œæˆï¼Œç»ˆç«¯ä¿æŒå¼€å¯...")
    logger.info("ğŸ“‹ æœ€ç»ˆç»Ÿè®¡æ‘˜è¦:")
    logger.info(f"  - è®­ç»ƒè½®æ•°: 100 (å®Œæ•´è®­ç»ƒï¼Œæ— æ—©åœ)")
    logger.info(f"  - æœ€ä½³éªŒè¯F1: {best_valid_f1:.4f}")
    logger.info(f"  - æµ‹è¯•F1: {test_f1:.4f}")
    logger.info(f"  - æ¨¡å‹å‚æ•°: {total_params:,}")
    logger.info(f"  - ç®€åŒ–LIVABLEæ¶æ„: âœ…")

    # ç­‰å¾…ç”¨æˆ·è¾“å…¥ä»¥ä¿æŒç»ˆç«¯å¼€å¯
    try:
        input("æŒ‰Enteré”®é€€å‡º...")
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except:
        logger.info("ç¨‹åºæ­£å¸¸ç»“æŸ")

if __name__ == "__main__":
    main()
