#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»åŸå§‹æ•°æ®ç›´æ¥åˆ›å»ºå¤šåˆ†ç±»æ•°æ®é›†
"""

import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
from pathlib import Path

def create_multiclass_dataset_direct():
    """ç›´æ¥ä»åŸå§‹æ•°æ®åˆ›å»ºå¤šåˆ†ç±»æ•°æ®é›†"""
    
    print("ğŸ¯ ç›´æ¥åˆ›å»ºCWEå¤šåˆ†ç±»æ•°æ®é›†")
    print("=" * 60)
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    print("ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®...")
    df = pd.read_csv('../all_vul.csv')
    
    # 2. åˆ†æCWEåˆ†å¸ƒ
    print("\nğŸ“Š CWEåˆ†å¸ƒåˆ†æ:")
    cwe_counts = df['CWE ID'].value_counts()
    print(f"æ€»CWEç±»å‹æ•°: {len(cwe_counts)}")
    
    # 3. é€‰æ‹©ä¸»è¦CWEç±»å‹ï¼ˆæ ·æœ¬æ•°>=100ï¼‰
    min_samples = 100
    major_cwes = cwe_counts[cwe_counts >= min_samples]
    print(f"é€‰æ‹©æ ·æœ¬æ•°>={min_samples}çš„CWEç±»å‹: {len(major_cwes)}ä¸ª")
    
    # è¿‡æ»¤æ•°æ®
    df_filtered = df[df['CWE ID'].isin(major_cwes.index)].copy()
    print(f"è¿‡æ»¤åæ ·æœ¬æ•°: {len(df_filtered)}")
    
    # 4. åˆ›å»ºæ ‡ç­¾æ˜ å°„
    cwe_to_label = {cwe: i for i, cwe in enumerate(major_cwes.index)}
    label_to_cwe = {i: cwe for cwe, i in cwe_to_label.items()}
    
    print(f"\nğŸ·ï¸ æ ‡ç­¾æ˜ å°„ ({len(major_cwes)}ä¸ªç±»åˆ«):")
    for i, cwe in label_to_cwe.items():
        count = major_cwes[cwe]
        print(f"   æ ‡ç­¾ {i}: {cwe} ({count} æ ·æœ¬)")
    
    # 5. æ·»åŠ æ•°å­—æ ‡ç­¾
    df_filtered['multiclass_label'] = df_filtered['CWE ID'].map(cwe_to_label)
    
    # 6. ä¿å­˜æ ‡ç­¾æ˜ å°„
    mapping_info = {
        'cwe_to_label': cwe_to_label,
        'label_to_cwe': label_to_cwe,
        'num_classes': len(major_cwes),
        'min_samples_threshold': min_samples,
        'total_samples': len(df_filtered),
        'class_distribution': {str(i): int(major_cwes[cwe]) for i, cwe in label_to_cwe.items()}
    }
    
    with open('multiclass_label_mapping.json', 'w') as f:
        json.dump(mapping_info, f, indent=2)
    
    print(f"\nğŸ’¾ æ ‡ç­¾æ˜ å°„å·²ä¿å­˜: multiclass_label_mapping.json")
    
    return df_filtered, mapping_info

def create_simplified_multiclass_data(df_filtered, mapping_info):
    """åˆ›å»ºç®€åŒ–çš„å¤šåˆ†ç±»æ•°æ®"""
    
    print("\nğŸ”„ åˆ›å»ºç®€åŒ–çš„å¤šåˆ†ç±»è®­ç»ƒæ•°æ®...")
    
    # æŒ‰æ ‡ç­¾åˆ†å±‚æŠ½æ ·
    samples_by_label = {}
    for _, row in df_filtered.iterrows():
        label = row['multiclass_label']
        if label not in samples_by_label:
            samples_by_label[label] = []
        samples_by_label[label].append(row)
    
    print(f"   å„ç±»åˆ«æ ·æœ¬æ•°:")
    for label in sorted(samples_by_label.keys()):
        cwe_id = mapping_info['label_to_cwe'][label]
        count = len(samples_by_label[label])
        print(f"     æ ‡ç­¾ {label} ({cwe_id}): {count} æ ·æœ¬")
    
    # åˆ†å±‚åˆ†å‰²æ•°æ®
    train_data, valid_data, test_data = [], [], []
    
    for label, samples in samples_by_label.items():
        if len(samples) < 3:
            # æ ·æœ¬å¤ªå°‘ï¼Œå…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
            train_data.extend(samples)
        else:
            # åˆ†å±‚åˆ†å‰²
            train_samples, temp_samples = train_test_split(
                samples, test_size=0.3, random_state=42
            )
            if len(temp_samples) >= 2:
                valid_samples, test_samples = train_test_split(
                    temp_samples, test_size=0.5, random_state=42
                )
            else:
                valid_samples = temp_samples[:1] if temp_samples else []
                test_samples = temp_samples[1:] if len(temp_samples) > 1 else []
            
            train_data.extend(train_samples)
            valid_data.extend(valid_samples)
            test_data.extend(test_samples)
    
    splits = {
        'train': train_data,
        'valid': valid_data,
        'test': test_data
    }
    
    print(f"\nğŸ“Š åˆ†å‰²ç»“æœ:")
    for split_name, split_data in splits.items():
        labels = [s['multiclass_label'] for s in split_data]
        label_counts = Counter(labels)
        print(f"   {split_name}: {len(split_data)} æ ·æœ¬")
        print(f"     æ ‡ç­¾åˆ†å¸ƒ: {dict(sorted(label_counts.items()))}")
    
    # è½¬æ¢ä¸ºç®€åŒ–çš„LIVABLEæ ¼å¼
    output_dir = Path("livable_multiclass_data")
    output_dir.mkdir(exist_ok=True)
    
    for split_name, split_data in splits.items():
        print(f"\nğŸ”„ è½¬æ¢ {split_name} æ•°æ®...")
        
        livable_samples = []
        for i, row in enumerate(split_data):
            if i % 1000 == 0 and i > 0:
                print(f"   è¿›åº¦: {i}/{len(split_data)}")
            
            livable_sample = create_simplified_sample(row)
            if livable_sample:
                livable_samples.append(livable_sample)
        
        # ä¿å­˜
        output_file = output_dir / f"livable_{split_name}.json"
        with open(output_file, 'w') as f:
            json.dump(livable_samples, f, indent=2, ensure_ascii=False)
        
        file_size_mb = output_file.stat().st_size / (1024 * 1024)
        print(f"   âœ… {split_name} è½¬æ¢å®Œæˆ: {len(livable_samples)} æ ·æœ¬ -> {output_file} ({file_size_mb:.1f}MB)")
    
    return output_dir

def create_simplified_sample(row):
    """åˆ›å»ºç®€åŒ–çš„æ ·æœ¬ï¼ˆåŸºäºä»£ç æ–‡æœ¬ç‰¹å¾ï¼‰"""
    
    try:
        # è·å–åŸºæœ¬ä¿¡æ¯
        multiclass_label = row['multiclass_label']
        func_code = str(row.get('processed_func', ''))
        cwe_id = row['CWE ID']
        
        if not func_code or func_code == 'nan':
            return None
        
        # åˆ›å»ºç®€åŒ–çš„ä»£ç ç‰¹å¾
        code_features = create_code_features(func_code)
        
        # åˆ›å»ºç®€åŒ–çš„å›¾ç»“æ„
        graph_data = create_simple_graph(func_code, code_features)
        
        # æ„å»ºLIVABLEæ ·æœ¬
        livable_sample = {
            'features': graph_data['node_features'],
            'structure': graph_data['edges'],
            'label': [[int(multiclass_label)]],
            'sequence': graph_data['sequence'],
            'metadata': {
                'id': int(row.get('index', 0)),
                'cwe_id': cwe_id,
                'multiclass_label': int(multiclass_label),
                'num_nodes': len(graph_data['node_features']),
                'num_edges': len(graph_data['edges']),
                'code_length': len(func_code)
            }
        }
        
        return livable_sample
        
    except Exception as e:
        print(f"âš ï¸ æ ·æœ¬è½¬æ¢å¤±è´¥: {e}")
        return None

def create_code_features(func_code):
    """ä»ä»£ç æ–‡æœ¬åˆ›å»ºç‰¹å¾å‘é‡"""
    
    # åŸºç¡€ä»£ç ç»Ÿè®¡ç‰¹å¾
    features = []
    
    # é•¿åº¦ç‰¹å¾
    features.append(len(func_code) / 1000.0)  # ä»£ç é•¿åº¦ï¼ˆå½’ä¸€åŒ–ï¼‰
    features.append(len(func_code.split()) / 100.0)  # è¯æ±‡æ•°é‡
    features.append(len(func_code.split('\\n')) / 50.0)  # è¡Œæ•°
    
    # å…³é”®è¯ç‰¹å¾
    keywords = ['if', 'for', 'while', 'return', 'malloc', 'free', 'strcpy', 'strcat', 'sprintf', 'gets']
    for keyword in keywords:
        features.append(func_code.lower().count(keyword) / 10.0)
    
    # ç¬¦å·ç‰¹å¾
    symbols = ['{', '}', '(', ')', '[', ']', ';', '*', '&', '->']
    for symbol in symbols:
        features.append(func_code.count(symbol) / 20.0)
    
    # å¡«å……åˆ°768ç»´
    while len(features) < 768:
        # ä½¿ç”¨ä»£ç å“ˆå¸Œç”Ÿæˆä¼ªéšæœºç‰¹å¾
        hash_val = hash(func_code + str(len(features))) % 10000
        features.append(hash_val / 10000.0)
    
    return features[:768]

def create_simple_graph(func_code, code_features):
    """åˆ›å»ºç®€åŒ–çš„å›¾ç»“æ„"""
    
    # åŸºäºä»£ç è¡Œåˆ›å»ºèŠ‚ç‚¹
    lines = func_code.split('\\n')
    max_nodes = min(50, max(5, len(lines)))  # 5-50ä¸ªèŠ‚ç‚¹
    
    node_features = []
    edges = []
    
    # åˆ›å»ºèŠ‚ç‚¹ç‰¹å¾
    for i in range(max_nodes):
        if i < len(lines):
            line = lines[i].strip()
            node_feature = create_line_features(line, code_features)
        else:
            node_feature = [0.0] * 768
        
        node_features.append(node_feature)
    
    # åˆ›å»ºè¾¹ï¼ˆç®€å•çš„é¡ºåºè¿æ¥ + ä¸€äº›éšæœºè¿æ¥ï¼‰
    for i in range(max_nodes - 1):
        # é¡ºåºè¿æ¥
        edges.append([i, 'AST', i + 1])
        
        # æ·»åŠ ä¸€äº›æ§åˆ¶æµè¾¹
        if i % 3 == 0 and i + 2 < max_nodes:
            edges.append([i, 'CFG', i + 2])
        
        # æ·»åŠ ä¸€äº›æ•°æ®æµè¾¹
        if i % 5 == 0 and i + 3 < max_nodes:
            edges.append([i, 'DFG', i + 3])
    
    # åˆ›å»ºåºåˆ—ç‰¹å¾
    sequence = []
    for i in range(6):  # 6ä¸ªæ—¶é—´æ­¥
        start_idx = i * 128
        end_idx = min(start_idx + 128, 768)
        seq_step = code_features[start_idx:end_idx]
        
        if len(seq_step) < 128:
            seq_step.extend([0.0] * (128 - len(seq_step)))
        
        sequence.append(seq_step)
    
    return {
        'node_features': node_features,
        'edges': edges,
        'sequence': sequence
    }

def create_line_features(line, base_features):
    """ä¸ºä»£ç è¡Œåˆ›å»ºç‰¹å¾"""
    
    features = base_features.copy()
    
    # è¡Œç‰¹å®šç‰¹å¾
    features[0] = len(line) / 100.0
    features[1] = line.count(' ') / 20.0
    features[2] = 1.0 if '{' in line else 0.0
    features[3] = 1.0 if '}' in line else 0.0
    features[4] = 1.0 if 'if' in line.lower() else 0.0
    
    # æ·»åŠ ä¸€äº›å™ªå£°ä»¥å¢åŠ å¤šæ ·æ€§
    for i in range(5, min(20, len(features))):
        noise = (hash(line + str(i)) % 1000) / 1000.0 * 0.1
        features[i] += noise
    
    return features

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ¯ ä»åŸå§‹æ•°æ®åˆ›å»ºå¤šåˆ†ç±»æ•°æ®é›†")
    print("åŸºäºCWE IDçš„æ¼æ´ç±»å‹åˆ†ç±»")
    print()
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(42)
    
    # 1. åˆ›å»ºå¤šåˆ†ç±»æ•°æ®é›†
    df_filtered, mapping_info = create_multiclass_dataset_direct()
    
    # 2. åˆ›å»ºç®€åŒ–çš„è®­ç»ƒæ•°æ®
    output_dir = create_simplified_multiclass_data(df_filtered, mapping_info)
    
    print(f"\nğŸ‰ å¤šåˆ†ç±»æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ·ï¸ ç±»åˆ«æ•°é‡: {mapping_info['num_classes']}")
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {mapping_info['total_samples']}")
    
    # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
    print(f"\nğŸ“Š æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ:")
    for i, cwe in mapping_info['label_to_cwe'].items():
        count = mapping_info['class_distribution'][str(i)]
        percentage = count / mapping_info['total_samples'] * 100
        print(f"   æ ‡ç­¾ {i}: {cwe} - {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    return mapping_info

if __name__ == "__main__":
    main()
