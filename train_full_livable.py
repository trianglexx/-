#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´LIVABLEæ¶æ„å¤šåˆ†ç±»è®­ç»ƒè„šæœ¬
ä½¿ç”¨åŸå§‹LIVABLEçš„å®Œæ•´æ¶æ„ï¼Œä¸ä½¿ç”¨æ—©åœæœºåˆ¶
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from datetime import datetime
import logging
from pathlib import Path
import torch.nn.functional as F
from tqdm import tqdm
import copy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# æ·»åŠ LIVABLEä»£ç è·¯å¾„
sys.path.append('LIVABLE-main/code')
from modules.model import DevignModel
from mlp_readout import MLPReadout
from data_loader.batch_graph import GGNNBatchGraph
from appnpconv import APPNPConv

try:
    import dgl
    from dgl import DGLGraph
    DGL_AVAILABLE = True
except ImportError:
    logger.error("DGLä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨å®Œæ•´LIVABLEæ¶æ„")
    sys.exit(1)

class FullLIVABLEDataEntry:
    """å®Œæ•´LIVABLEæ•°æ®æ¡ç›®ç±»"""
    def __init__(self, features, edges, target, sequence, edge_types):
        self.num_nodes = len(features)
        self.target = target
        self.graph = DGLGraph()
        
        # å¤„ç†ç‰¹å¾
        self.features = torch.FloatTensor(features)
        
        self.graph.add_nodes(self.num_nodes, data={'features': self.features})
        self.seq = torch.FloatTensor(sequence)
        self.edge_types = edge_types
        
        # æ·»åŠ è¾¹
        sources = []
        dests = []
        etypes = []
        
        for s, edge_type, t in edges:
            if s < self.num_nodes and t < self.num_nodes:
                sources.append(s)
                dests.append(t)
                etype_number = self.get_edge_type_number(edge_type)
                etypes.append(etype_number)
        
        if sources:  # åªæœ‰å½“æœ‰è¾¹æ—¶æ‰æ·»åŠ 
            self.graph.add_edges(sources, dests, data={'etype': torch.LongTensor(etypes)})
    
    def get_edge_type_number(self, edge_type):
        """è·å–è¾¹ç±»å‹ç¼–å·"""
        if edge_type not in self.edge_types:
            self.edge_types[edge_type] = len(self.edge_types)
        return self.edge_types[edge_type]

class FullLIVABLEDataset(Dataset):
    """å®Œæ•´LIVABLEæ•°æ®é›†ç±»"""
    def __init__(self, data_path, max_seq_len=512):
        self.data = []
        self.edge_types = {}
        self.max_etype = 0
        self.max_seq_len = max_seq_len
        
        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {data_path}")
        with open(data_path, 'r') as f:
            raw_data = json.load(f)
        
        logger.info(f"ğŸ“Š å¤„ç† {len(raw_data)} ä¸ªæ ·æœ¬...")
        for item in tqdm(raw_data, desc="å¤„ç†æ•°æ®"):
            try:
                features = item['features']
                edges = item['structure']
                target = item['label'][0][0]
                sequence = item['sequence']
                
                # å¤„ç†åºåˆ—é•¿åº¦
                if len(sequence) > self.max_seq_len:
                    sequence = sequence[:self.max_seq_len]
                else:
                    # ç”¨é›¶å‘é‡å¡«å……
                    if len(sequence) > 0:
                        feat_dim = len(sequence[0])
                        padding = [[0.0] * feat_dim] * (self.max_seq_len - len(sequence))
                        sequence = sequence + padding
                    else:
                        sequence = [[0.0] * 128] * self.max_seq_len
                
                data_entry = FullLIVABLEDataEntry(features, edges, target, sequence, self.edge_types)
                self.data.append(data_entry)
                
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ ·æœ¬: {e}")
                continue
        
        self.max_etype = len(self.edge_types)
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} ä¸ªæ ·æœ¬")
        logger.info(f"ğŸ”— è¾¹ç±»å‹æ•°é‡: {self.max_etype}")
        
        # è·å–åºåˆ—ç‰¹å¾ç»´åº¦
        if len(self.data) > 0:
            self.seq_feature_dim = len(self.data[0].seq[0])
            logger.info(f"ğŸ“ åºåˆ—ç‰¹å¾ç»´åº¦: {self.seq_feature_dim}")
        else:
            self.seq_feature_dim = 128
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

class FullLIVABLEModel(nn.Module):
    """å®Œæ•´LIVABLEæ¨¡å‹ï¼ŒåŸºäºåŸå§‹DevignModel"""
    def __init__(self, input_dim=768, seq_input_dim=128, num_classes=14, max_edge_types=10, batch_size=16):
        super(FullLIVABLEModel, self).__init__()
        
        self.input_dim = input_dim
        self.seq_input_dim = seq_input_dim
        self.num_classes = num_classes
        self.max_edge_types = max_edge_types
        self.batch_size = batch_size
        
        # APPNPå‚æ•°
        k = 16
        alpha = 0.1
        self.hidden_dim = 128
        self.hidden_dim2 = 256
        
        # APPNPå±‚
        self.appnp = APPNPConv(k=k, alpha=alpha, edge_drop=0.5)
        
        # å›¾åˆ†æ”¯çš„GRU
        self.num_layers = 1
        self.bigru = nn.GRU(self.input_dim, self.hidden_dim, num_layers=self.num_layers, 
                           bidirectional=True, batch_first=True)
        
        # åºåˆ—åˆ†æ”¯çš„GRU
        self.seq_hid = 512
        self.bigru1 = nn.GRU(self.seq_input_dim, self.seq_hid, num_layers=self.num_layers,
                            bidirectional=True, batch_first=True)
        
        # åˆ†ç±»å™¨
        self.MPL_layer = MLPReadout(self.hidden_dim2, num_classes)
        self.MPL_layer1 = MLPReadout(2 * self.seq_hid, num_classes)
        
        # Dropout
        self.dropout = torch.nn.Dropout(0.2)
        self.dropout1 = torch.nn.Dropout(0.2)
        
    def init_hidden(self, batch_size):
        """åˆå§‹åŒ–å›¾åˆ†æ”¯éšè—çŠ¶æ€"""
        return torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).cuda()
    
    def init_hidden1(self, batch_size):
        """åˆå§‹åŒ–åºåˆ—åˆ†æ”¯éšè—çŠ¶æ€"""
        return torch.zeros(self.num_layers * 2, batch_size, self.seq_hid).cuda()
        
    def forward(self, batch_graph, sequences):
        """å‰å‘ä¼ æ’­"""
        batch_size = sequences.size(0)
        
        # è·å–ç½‘ç»œè¾“å…¥
        graph, features, edge_types = batch_graph.get_network_inputs(cuda=True)
        
        # åºåˆ—åˆ†æ”¯
        hidden1 = self.init_hidden1(batch_size)
        seq, _ = self.bigru1(sequences, hidden1)
        seq = torch.transpose(seq, 1, 2)
        seq1 = F.avg_pool1d(seq, seq.size(2)).squeeze(2)
        seq2 = F.max_pool1d(seq, seq.size(2)).squeeze(2)
        
        # å›¾åˆ†æ”¯
        graph = graph.to(torch.device('cuda:0'))
        
        # è§£æ‰¹å¤„ç†
        st = batch_graph.de_batchify_graphs(features)
        
        # GRUå¤„ç†
        hidden = self.init_hidden(batch_size)
        st, _ = self.bigru(st, hidden)
        
        # é‡æ–°æ‰¹å¤„ç†
        features = batch_graph.en_batchify_graphs(st)
        
        # æ·»åŠ è‡ªç¯
        graph = dgl.add_self_loop(graph)
        
        # APPNPå¤„ç†
        features = self.appnp(graph, features)
        
        # å†æ¬¡è§£æ‰¹å¤„ç†
        st = batch_graph.de_batchify_graphs(features)
        
        # æ± åŒ–
        st = torch.transpose(st, 1, 2)
        st1 = F.max_pool1d(st, st.size(2)).squeeze(2)
        st2 = F.avg_pool1d(st, st.size(2)).squeeze(2)
        
        # åˆ†ç±»
        graph_outputs = self.MPL_layer(self.dropout(st1 + st2))
        seq_outputs = self.MPL_layer1(self.dropout1(seq1 + seq2))
        
        # èåˆè¾“å‡º
        final_outputs = graph_outputs + seq_outputs
        
        return final_outputs

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    batch_graph = GGNNBatchGraph()
    sequences = []
    targets = []
    
    for item in batch:
        batch_graph.add_subgraph(item.graph)
        sequences.append(item.seq)
        targets.append(item.target)
    
    # å¤„ç†åºåˆ—
    sequences = torch.stack(sequences)
    targets = torch.LongTensor(targets)
    
    return batch_graph, sequences, targets

def setup_training_environment():
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("full_livable_results")
    output_dir.mkdir(exist_ok=True)
    
    return device, output_dir

def load_datasets():
    """åŠ è½½è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†"""
    train_dataset = FullLIVABLEDataset('livable_multiclass_data/livable_train.json')
    valid_dataset = FullLIVABLEDataset('livable_multiclass_data/livable_valid.json')
    test_dataset = FullLIVABLEDataset('livable_multiclass_data/livable_test.json')
    
    return train_dataset, valid_dataset, test_dataset

def create_data_loaders(train_dataset, valid_dataset, test_dataset, batch_size=16):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    valid_loader = DataLoader(
        valid_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    return train_loader, valid_loader, test_loader

def evaluate_model(model, data_loader, device, criterion):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for batch_graph, sequences, targets in tqdm(data_loader, desc="è¯„ä¼°ä¸­"):
            sequences = sequences.to(device)
            targets = targets.to(device)

            try:
                # å‰å‘ä¼ æ’­
                outputs = model(batch_graph, sequences)
                loss = criterion(outputs, targets)

                total_loss += loss.item()

                # è·å–é¢„æµ‹ç»“æœ
                _, predicted = torch.max(outputs.data, 1)
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())

            except Exception as e:
                logger.warning(f"è¯„ä¼°æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                continue

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted', zero_division=0
    )

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracies = {}
    conf_matrix = confusion_matrix(all_targets, all_predictions)
    for i in range(len(conf_matrix)):
        if conf_matrix[i].sum() > 0:
            class_accuracies[i] = conf_matrix[i][i] / conf_matrix[i].sum()
        else:
            class_accuracies[i] = 0.0

    avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else 0

    return avg_loss, accuracy, precision, recall, f1, all_predictions, all_targets, class_accuracies

def train_model(model, train_loader, valid_loader, device, num_epochs=100, learning_rate=0.001):
    """è®­ç»ƒæ¨¡å‹ - ä¸ä½¿ç”¨æ—©åœ"""
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

    best_valid_f1 = 0
    best_model_state = None

    training_history = {
        'train_loss': [],
        'train_acc': [],
        'valid_loss': [],
        'valid_acc': [],
        'valid_f1': []
    }

    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepochï¼ˆä¸ä½¿ç”¨æ—©åœï¼‰")

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_graph, sequences, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            sequences = sequences.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()

            try:
                # å‰å‘ä¼ æ’­
                outputs = model(batch_graph, sequences)
                loss = criterion(outputs, targets)

                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += targets.size(0)
                train_correct += (predicted == targets).sum().item()

            except Exception as e:
                logger.warning(f"è®­ç»ƒæ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
                continue

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_accuracy = train_correct / train_total if train_total > 0 else 0
        avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0

        # éªŒè¯é˜¶æ®µ
        valid_loss, valid_accuracy, valid_precision, valid_recall, valid_f1, _, _, _ = evaluate_model(
            model, valid_loader, device, criterion
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(valid_f1)

        # è®°å½•å†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['train_acc'].append(train_accuracy)
        training_history['valid_loss'].append(valid_loss)
        training_history['valid_acc'].append(valid_accuracy)
        training_history['valid_f1'].append(valid_f1)

        logger.info(f"Epoch {epoch+1}/{num_epochs}:")
        logger.info(f"  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}")
        logger.info(f"  éªŒè¯ - Loss: {valid_loss:.4f}, Acc: {valid_accuracy:.4f}, F1: {valid_f1:.4f}")
        logger.info(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä½†ä¸æ—©åœï¼‰
        if valid_f1 > best_valid_f1:
            best_valid_f1 = valid_f1
            best_model_state = copy.deepcopy(model.state_dict())
            logger.info(f"  âœ… æ–°çš„æœ€ä½³F1åˆ†æ•°: {best_valid_f1:.4f}")

    # æ¢å¤æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    logger.info(f"ğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯F1: {best_valid_f1:.4f}")

    return model, training_history, best_valid_f1

def save_results(model, training_history, test_results, output_dir):
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / "best_full_livable_model.pth"
    torch.save(model.state_dict(), model_path)
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = output_dir / "full_livable_training_history.json"
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2)
    logger.info(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

    # ä¿å­˜æµ‹è¯•ç»“æœ
    results_path = output_dir / "full_livable_final_results.json"
    with open(results_path, 'w') as f:
        json.dump(test_results, f, indent=2)
    logger.info(f"ğŸ“ˆ æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¼€å§‹å®Œæ•´LIVABLEå¤šåˆ†ç±»æ¼æ´æ£€æµ‹è®­ç»ƒ")

    # è®¾ç½®ç¯å¢ƒ
    device, output_dir = setup_training_environment()

    # åŠ è½½æ•°æ®
    logger.info("ğŸ“¥ åŠ è½½æ•°æ®é›†...")
    train_dataset, valid_dataset, test_dataset = load_datasets()

    # è·å–è¾¹ç±»å‹æ•°é‡å’Œåºåˆ—ç‰¹å¾ç»´åº¦
    max_edge_types = max(train_dataset.max_etype, valid_dataset.max_etype, test_dataset.max_etype)
    seq_feature_dim = train_dataset.seq_feature_dim
    logger.info(f"ğŸ”— æœ€å¤§è¾¹ç±»å‹æ•°: {max_edge_types}")
    logger.info(f"ğŸ“ åºåˆ—ç‰¹å¾ç»´åº¦: {seq_feature_dim}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 16
    train_loader, valid_loader, test_loader = create_data_loaders(
        train_dataset, valid_dataset, test_dataset, batch_size=batch_size
    )

    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ—ï¸ åˆ›å»ºå®Œæ•´LIVABLEæ¨¡å‹...")
    model = FullLIVABLEModel(
        input_dim=768,
        seq_input_dim=seq_feature_dim,
        num_classes=14,
        max_edge_types=max_edge_types,
        batch_size=batch_size
    ).to(device)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")

    # è®­ç»ƒæ¨¡å‹ï¼ˆä¸ä½¿ç”¨æ—©åœï¼‰
    model, training_history, best_valid_f1 = train_model(
        model, train_loader, valid_loader, device, num_epochs=100, learning_rate=0.001
    )

    # æµ‹è¯•æ¨¡å‹
    logger.info("ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    criterion = nn.CrossEntropyLoss()
    test_loss, test_accuracy, test_precision, test_recall, test_f1, predictions, targets, class_accuracies = evaluate_model(
        model, test_loader, device, criterion
    )

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    with open('multiclass_label_mapping.json', 'r') as f:
        label_mapping = json.load(f)

    class_names = [label_mapping['label_to_cwe'][str(i)] for i in range(14)]
    classification_rep = classification_report(
        targets, predictions, target_names=class_names, output_dict=True, zero_division=0
    )

    # æ··æ·†çŸ©é˜µ
    conf_matrix = confusion_matrix(targets, predictions)

    # æ•´ç†æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracy_dict = {}
    for i, class_name in enumerate(class_names):
        class_accuracy_dict[class_name] = class_accuracies.get(i, 0.0)

    # æ•´ç†æµ‹è¯•ç»“æœ
    test_results = {
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'best_valid_f1': best_valid_f1,
        'classification_report': classification_rep,
        'confusion_matrix': conf_matrix.tolist(),
        'class_names': class_names,
        'class_accuracies': class_accuracy_dict,
        'model_info': {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'input_dim': 768,
            'seq_input_dim': seq_feature_dim,
            'num_classes': 14,
            'max_edge_types': max_edge_types,
            'batch_size': batch_size,
            'epochs_trained': 100,
            'early_stopping': False
        }
    }

    # æ‰“å°ç»“æœ
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    logger.info(f"  å‡†ç¡®ç‡: {test_accuracy:.4f}")
    logger.info(f"  ç²¾ç¡®ç‡: {test_precision:.4f}")
    logger.info(f"  å¬å›ç‡: {test_recall:.4f}")
    logger.info(f"  F1åˆ†æ•°: {test_f1:.4f}")

    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    logger.info("ğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡:")
    for class_name, acc in class_accuracy_dict.items():
        logger.info(f"  {class_name}: {acc:.4f}")

    # ä¿å­˜ç»“æœ
    save_results(model, training_history, test_results, output_dir)

    logger.info("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜å®Œæˆï¼")

    # ä¿æŒç»ˆç«¯å¼€å¯ï¼Œæ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    logger.info("ğŸ”„ è®­ç»ƒå®Œæˆï¼Œç»ˆç«¯ä¿æŒå¼€å¯...")
    logger.info("ğŸ“‹ æœ€ç»ˆç»Ÿè®¡æ‘˜è¦:")
    logger.info(f"  - è®­ç»ƒè½®æ•°: 100 (å®Œæ•´è®­ç»ƒï¼Œæ— æ—©åœ)")
    logger.info(f"  - æœ€ä½³éªŒè¯F1: {best_valid_f1:.4f}")
    logger.info(f"  - æµ‹è¯•F1: {test_f1:.4f}")
    logger.info(f"  - æ¨¡å‹å‚æ•°: {total_params:,}")
    logger.info(f"  - å®Œæ•´LIVABLEæ¶æ„: âœ…")

    # ç­‰å¾…ç”¨æˆ·è¾“å…¥ä»¥ä¿æŒç»ˆç«¯å¼€å¯
    try:
        input("æŒ‰Enteré”®é€€å‡º...")
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except:
        logger.info("ç¨‹åºæ­£å¸¸ç»“æŸ")

if __name__ == "__main__":
    main()
