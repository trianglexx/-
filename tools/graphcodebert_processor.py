#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GraphCodeBERTå¤„ç†å™¨
å°†CodeBERTæ›¿æ¢ä¸ºGraphCodeBERTï¼Œæä¾›æ›´å¥½çš„ä»£ç ç†è§£èƒ½åŠ›
"""

# Hugging Faceé•œåƒé…ç½®
import os
if 'HF_ENDPOINT' not in os.environ:
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ['HUGGINGFACE_HUB_CACHE'] = '/root/.cache/huggingface'

import json
import logging
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime

import torch
import torch.nn as nn
from transformers import (
    RobertaTokenizer, 
    RobertaModel,
    AutoTokenizer,
    AutoModel
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GraphCodeBERTProcessor:
    """GraphCodeBERTå¤„ç†å™¨"""
    
    def __init__(self, model_name: str = "microsoft/graphcodebert-base"):
        """
        åˆå§‹åŒ–GraphCodeBERTå¤„ç†å™¨
        
        Args:
            model_name: GraphCodeBERTæ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"ğŸ¤– åˆå§‹åŒ–GraphCodeBERTå¤„ç†å™¨...")
        logger.info(f"   æ¨¡å‹: {model_name}")
        logger.info(f"   è®¾å¤‡: {self.device}")
        
        # åŠ è½½tokenizerå’Œmodel
        self.tokenizer = None
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½GraphCodeBERTæ¨¡å‹"""
        try:
            logger.info("ğŸ“¥ åŠ è½½GraphCodeBERT tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            logger.info("ğŸ“¥ åŠ è½½GraphCodeBERT model...")
            self.model = AutoModel.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_safetensors=True  # ä½¿ç”¨safetensorsæ ¼å¼é¿å…torch.loadé—®é¢˜
            ).to(self.device)
            
            self.model.eval()
            
            logger.info("âœ… GraphCodeBERTæ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"   è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size}")
            logger.info(f"   éšè—å±‚ç»´åº¦: {self.model.config.hidden_size}")
            
        except Exception as e:
            logger.error(f"âŒ GraphCodeBERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode_code(self, code: str, max_length: int = 512) -> np.ndarray:
        """
        ä½¿ç”¨GraphCodeBERTç¼–ç ä»£ç 
        
        Args:
            code: æºä»£ç å­—ç¬¦ä¸²
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            
        Returns:
            ä»£ç åµŒå…¥å‘é‡ (768ç»´)
        """
        try:
            # é¢„å¤„ç†ä»£ç 
            code = self._preprocess_code(code)
            
            # Tokenize
            inputs = self.tokenizer(
                code,
                max_length=max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # è·å–åµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                # ä½¿ç”¨[CLS] tokençš„åµŒå…¥ä½œä¸ºä»£ç è¡¨ç¤º
                code_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return code_embedding.flatten()
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä»£ç ç¼–ç å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            return np.zeros(768, dtype=np.float32)
    
    def encode_code_with_dfg(self, code: str, dfg_edges: List[Tuple] = None, 
                            max_length: int = 512) -> np.ndarray:
        """
        ä½¿ç”¨GraphCodeBERTç¼–ç ä»£ç å’Œæ•°æ®æµå›¾
        
        Args:
            code: æºä»£ç å­—ç¬¦ä¸²
            dfg_edges: æ•°æ®æµå›¾è¾¹åˆ—è¡¨ [(source, target, relation)]
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            
        Returns:
            å¢å¼ºçš„ä»£ç åµŒå…¥å‘é‡ (768ç»´)
        """
        try:
            # é¢„å¤„ç†ä»£ç 
            code = self._preprocess_code(code)
            
            # å¦‚æœæœ‰DFGä¿¡æ¯ï¼Œæ„å»ºå¢å¼ºè¾“å…¥
            if dfg_edges:
                # æ„å»ºDFGå¢å¼ºçš„è¾“å…¥
                enhanced_input = self._build_dfg_enhanced_input(code, dfg_edges)
            else:
                enhanced_input = code
            
            # Tokenize
            inputs = self.tokenizer(
                enhanced_input,
                max_length=max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            ).to(self.device)
            
            # è·å–åµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                # ä½¿ç”¨[CLS] tokençš„åµŒå…¥ä½œä¸ºä»£ç è¡¨ç¤º
                code_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return code_embedding.flatten()
            
        except Exception as e:
            logger.warning(f"âš ï¸ DFGå¢å¼ºç¼–ç å¤±è´¥: {e}")
            # å›é€€åˆ°æ™®é€šç¼–ç 
            return self.encode_code(code, max_length)
    
    def _preprocess_code(self, code: str) -> str:
        """é¢„å¤„ç†ä»£ç """
        if not code or not isinstance(code, str):
            return ""
        
        # ç§»é™¤è¿‡å¤šçš„ç©ºç™½å­—ç¬¦
        code = ' '.join(code.split())
        
        # é™åˆ¶é•¿åº¦
        if len(code) > 2000:
            code = code[:2000]
        
        return code
    
    def _build_dfg_enhanced_input(self, code: str, dfg_edges: List[Tuple]) -> str:
        """æ„å»ºDFGå¢å¼ºçš„è¾“å…¥"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå°†DFGä¿¡æ¯ä½œä¸ºæ³¨é‡Šæ·»åŠ åˆ°ä»£ç ä¸­
        dfg_info = []
        for edge in dfg_edges[:10]:  # é™åˆ¶DFGè¾¹æ•°é‡
            if len(edge) >= 3:
                source, target, relation = edge[0], edge[1], edge[2]
                dfg_info.append(f"{source}->{target}({relation})")
        
        if dfg_info:
            dfg_comment = "// DFG: " + ", ".join(dfg_info)
            enhanced_input = f"{dfg_comment}\n{code}"
        else:
            enhanced_input = code
        
        return enhanced_input
    
    def batch_encode_codes(self, codes: List[str], max_length: int = 512) -> np.ndarray:
        """
        æ‰¹é‡ç¼–ç ä»£ç 
        
        Args:
            codes: ä»£ç åˆ—è¡¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            
        Returns:
            ä»£ç åµŒå…¥çŸ©é˜µ [batch_size, 768]
        """
        embeddings = []
        
        logger.info(f"ğŸ”„ æ‰¹é‡ç¼–ç  {len(codes)} ä¸ªä»£ç æ ·æœ¬...")
        
        for i, code in enumerate(codes):
            if i % 100 == 0:
                logger.info(f"   è¿›åº¦: {i}/{len(codes)}")
            
            embedding = self.encode_code(code, max_length)
            embeddings.append(embedding)
        
        logger.info("âœ… æ‰¹é‡ç¼–ç å®Œæˆ")
        return np.array(embeddings)
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "vocab_size": self.tokenizer.vocab_size if self.tokenizer else 0,
            "hidden_size": self.model.config.hidden_size if self.model else 0,
            "max_position_embeddings": getattr(self.model.config, 'max_position_embeddings', 512),
            "device": str(self.device)
        }

def migrate_codebert_to_graphcodebert(input_file: str, output_file: str):
    """
    å°†CodeBERTåµŒå…¥è¿ç§»åˆ°GraphCodeBERT
    
    Args:
        input_file: åŒ…å«CodeBERTåµŒå…¥çš„è¾“å…¥æ–‡ä»¶
        output_file: GraphCodeBERTåµŒå…¥çš„è¾“å‡ºæ–‡ä»¶
    """
    logger.info("ğŸ”„ å¼€å§‹CodeBERTåˆ°GraphCodeBERTçš„è¿ç§»...")
    
    # åˆå§‹åŒ–GraphCodeBERTå¤„ç†å™¨
    processor = GraphCodeBERTProcessor()
    
    # åŠ è½½æ•°æ®
    logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"   æ ·æœ¬æ•°é‡: {len(data)}")
    
    # å¤„ç†æ¯ä¸ªæ ·æœ¬
    updated_data = []
    for i, sample in enumerate(data):
        if i % 50 == 0:
            logger.info(f"   å¤„ç†è¿›åº¦: {i}/{len(data)}")
        
        # è·å–åŸå§‹ä»£ç 
        code = sample.get('func', '') or sample.get('original_code', '')
        
        if code:
            # ä½¿ç”¨GraphCodeBERTé‡æ–°ç¼–ç 
            new_embedding = processor.encode_code(code)
            
            # æ›´æ–°åµŒå…¥
            sample['code_embedding'] = new_embedding.tolist()
            sample['embedding_model'] = 'graphcodebert-base'
            sample['embedding_dim'] = len(new_embedding)
        else:
            logger.warning(f"   æ ·æœ¬ {i} æ²¡æœ‰ä»£ç å†…å®¹ï¼Œè·³è¿‡")
        
        updated_data.append(sample)
    
    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
    logger.info(f"ğŸ’¾ ä¿å­˜æ›´æ–°æ•°æ®: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(updated_data, f, indent=2, ensure_ascii=False)
    
    logger.info("âœ… è¿ç§»å®Œæˆ!")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    model_info = processor.get_model_info()
    logger.info("ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    for key, value in model_info.items():
        logger.info(f"   {key}: {value}")

def test_graphcodebert():
    """æµ‹è¯•GraphCodeBERTå¤„ç†å™¨"""
    logger.info("ğŸ§ª æµ‹è¯•GraphCodeBERTå¤„ç†å™¨...")
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = GraphCodeBERTProcessor()
    
    # æµ‹è¯•ä»£ç 
    test_code = """
    int vulnerable_function(char* input) {
        char buffer[100];
        strcpy(buffer, input);  // Buffer overflow vulnerability
        return strlen(buffer);
    }
    """
    
    # ç¼–ç æµ‹è¯•
    embedding = processor.encode_code(test_code)
    
    logger.info(f"âœ… æµ‹è¯•æˆåŠŸ!")
    logger.info(f"   åµŒå…¥ç»´åº¦: {embedding.shape}")
    logger.info(f"   åµŒå…¥èŒƒå›´: [{embedding.min():.4f}, {embedding.max():.4f}]")
    logger.info(f"   åµŒå…¥å‡å€¼: {embedding.mean():.4f}")
    
    return True

if __name__ == "__main__":
    # æµ‹è¯•GraphCodeBERTå¤„ç†å™¨
    test_graphcodebert()
