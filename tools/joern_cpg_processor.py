#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæ­¥éª¤1: Joern CPGå¤„ç†
åŸºäºVulLMGNNçš„æˆåŠŸå®ç°ï¼Œä¿®å¤CPGç”Ÿæˆå’Œè§£æé—®é¢˜
"""

import os
import sys
import json
import argparse
import time
import subprocess
import hashlib
import re
import shutil
from typing import List, Dict, Any
from tqdm import tqdm

# æ·»åŠ VulLMGNNçš„utilsè·¯å¾„
sys.path.append('/root/autodl-tmp/vullmgnn/utils')
sys.path.append('/root/autodl-tmp/vullmgnn')

try:
    from utils.functions.cpg.complete_parser import parse_complete_cpg_to_nodes
    VULLMGNN_UTILS_AVAILABLE = True
    print("âœ… VulLMGNN utilså¯ç”¨")
except ImportError:
    print("âš ï¸ VulLMGNN utilsä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    VULLMGNN_UTILS_AVAILABLE = False


class JoernCPGProcessor:
    """Joern CPGå¤„ç†å™¨ - æ ‡å‡†æ¥å£"""

    def __init__(self, joern_path: str = "/root/autodl-tmp/vullmgnn/joern/joern-cli/"):
        """åˆå§‹åŒ–Joern CPGå¤„ç†å™¨"""
        self.joern_path = joern_path
        self.fixed_processor = None

    def process_csv_to_cpg(self, csv_file: str, output_file: str, batch_size: int = 10) -> bool:
        """å¤„ç†CSVæ–‡ä»¶åˆ°CPGæ ¼å¼"""
        try:
            # é¦–å…ˆå°†CSVè½¬æ¢ä¸ºJSONæ ¼å¼
            json_file = self._convert_csv_to_json(csv_file, batch_size)

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_file) if output_file else "./joern_output"
            if not output_dir:
                output_dir = "./joern_output"

            # ä½¿ç”¨å†…éƒ¨çš„FixedJoernProcessor
            self.fixed_processor = FixedJoernProcessor(
                input_json=json_file,
                output_dir=output_dir,
                joern_path=self.joern_path,
                batch_size=batch_size,
                max_samples=batch_size,  # é™åˆ¶æ ·æœ¬æ•°é‡ç”¨äºæµ‹è¯•
                timeout_seconds=30,
                verbose=True
            )

            # æ‰§è¡Œå¤„ç†
            self.fixed_processor.run_processing()
            return True

        except Exception as e:
            print(f"âŒ Joern CPGå¤„ç†å¤±è´¥: {e}")
            return False

    def _convert_csv_to_json(self, csv_file: str, max_samples: int) -> str:
        """å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼"""
        import pandas as pd

        print(f"ğŸ“Š è½¬æ¢CSVåˆ°JSONæ ¼å¼: {csv_file}")

        # è¯»å–CSV
        df = pd.read_csv(csv_file)

        # æ‰¾åˆ°ä»£ç åˆ—
        func_col = None
        for col in ['func', 'processed_func', 'func_before', 'vul_func_with_fix']:
            if col in df.columns:
                func_col = col
                break

        if func_col is None:
            raise ValueError("CSVæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»£ç åˆ—")

        # é‡‡æ ·æ•°æ®
        if len(df) > max_samples:
            df = df.sample(max_samples)

        # è½¬æ¢ä¸ºJSONæ ¼å¼
        json_data = []
        for i, (_, row) in enumerate(df.iterrows()):
            json_sample = {
                'id': i,
                'func': str(row[func_col]) if pd.notna(row[func_col]) else "",
                'target': int(row['target']) if 'target' in row and pd.notna(row['target']) else 0
            }
            json_data.append(json_sample)

        # ä¿å­˜ä¸´æ—¶JSONæ–‡ä»¶
        json_file = "temp_joern_input.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)

        print(f"âœ… è½¬æ¢å®Œæˆ: {len(json_data)} ä¸ªæ ·æœ¬")
        return json_file


class FixedJoernProcessor:
    """ä¿®å¤ç‰ˆJoern CPGå¤„ç†å™¨"""
    
    def __init__(self,
                 input_json: str,
                 output_dir: str = "./step1_output/",
                 joern_path: str = "/root/autodl-tmp/vullmgnn/joern/joern-cli/",
                 batch_size: int = 50,
                 max_samples: int = None,
                 timeout_seconds: int = 15,
                 verbose: bool = True):
        """
        åˆå§‹åŒ–ä¿®å¤ç‰ˆJoernå¤„ç†å™¨
        """
        self.input_json = input_json
        self.output_dir = output_dir
        self.joern_path = joern_path
        self.batch_size = batch_size
        self.max_samples = max_samples
        self.timeout_seconds = timeout_seconds
        self.verbose = verbose
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        self.temp_dir = os.path.join(self.output_dir, "temp")
        os.makedirs(self.temp_dir, exist_ok=True)
        
        # åˆ›å»ºCPGæ•°æ®è¾“å‡ºç›®å½•
        self.cpg_data_dir = os.path.join(self.output_dir, "cpg_data")
        os.makedirs(self.cpg_data_dir, exist_ok=True)
        
        # éªŒè¯Joernå·¥å…·
        self.joern_parse = os.path.join(joern_path, "joern-parse")
        self.joern_cli = os.path.join(joern_path, "joern")
        
        if not os.path.exists(self.joern_parse):
            raise FileNotFoundError(f"joern-parseæœªæ‰¾åˆ°: {self.joern_parse}")
        if not os.path.exists(self.joern_cli):
            raise FileNotFoundError(f"joernæœªæ‰¾åˆ°: {self.joern_cli}")
        
        # æ£€æŸ¥Joernè„šæœ¬
        self.joern_script = "/root/autodl-tmp/vullmgnn/joern/simple-cpg-extract.sc"
        if not os.path.exists(self.joern_script):
            raise FileNotFoundError(f"Joernè„šæœ¬æœªæ‰¾åˆ°: {self.joern_script}")
        
        if self.verbose:
            print("ğŸ”§ ä¿®å¤ç‰ˆJoern CPGå¤„ç†å™¨åˆå§‹åŒ–")
            print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {self.input_json}")
            print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            print(f"â±ï¸ è¶…æ—¶æ—¶é—´: {self.timeout_seconds}ç§’")
            if max_samples:
                print(f"ğŸ”¢ æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    def log(self, message: str):
        """æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            timestamp = time.strftime("%H:%M:%S")
            print(f"[{timestamp}] {message}")
    
    def get_code_hash(self, code: str) -> str:
        """è·å–ä»£ç çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(code.encode('utf-8')).hexdigest()[:16]
    
    def clean_code_for_joern(self, code: str) -> str:
        """æ¸…ç†ä»£ç ä»¥ç¡®ä¿Joernèƒ½æ­£ç¡®å¤„ç† - åŸºäºVulLMGNNçš„æ–¹æ³•"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        code = re.sub(r'\s+', ' ', code.strip())
        
        # ç¡®ä¿ä»£ç ä¸ä¸ºç©º
        if not code or len(code.strip()) == 0:
            code = "int main() { return 0; }"
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        code = re.sub(r'[^\x00-\x7F]+', ' ', code)
        
        # ç¡®ä¿ä»£ç é•¿åº¦åˆç†
        if len(code) > 2000:
            code = code[:2000]
        
        # ç¡®ä¿ä»£ç ä»¥åˆ†å·æˆ–å¤§æ‹¬å·ç»“å°¾
        if not code.rstrip().endswith((';', '}', '{')):
            code = code.rstrip() + ';'
        
        return code
    
    def load_and_clean_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½å¹¶æ¸…ç†æ•°æ®"""
        self.log("ğŸ“Š åŠ è½½å¹¶æ¸…ç†æ•°æ®...")
        
        with open(self.input_json, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        if self.max_samples:
            raw_data = raw_data[:self.max_samples]
        
        self.log(f"åŸå§‹æ•°æ®æ€»æ•°: {len(raw_data)}")
        
        # æ¸…ç†æ•°æ®
        cleaned_data = []
        
        for sample in raw_data:
            func_code = sample.get('func', '')
            target = sample.get('target', 0)
            
            # åŸºæœ¬è¿‡æ»¤æ¡ä»¶
            if (len(func_code.strip()) > 15 and 
                len(func_code) < 5000 and
                '{' in func_code and '}' in func_code and
                func_code.count('\n') < 200 and
                func_code.count(';') > 0):
                
                # æ¸…ç†ä»£ç 
                cleaned_code = self.clean_code_for_joern(func_code)
                
                cleaned_data.append({
                    'func': cleaned_code,
                    'original_func': func_code,
                    'target': target,
                    'metadata': sample.get('metadata', {}),
                    'index': len(cleaned_data),
                    'hash': self.get_code_hash(cleaned_code)
                })
        
        self.log(f"æ¸…æ´—åæ•°æ®: {len(cleaned_data)}/{len(raw_data)} ({len(cleaned_data)/len(raw_data)*100:.1f}%)")
        return cleaned_data
    
    def create_batch_c_files(self, batch_data: List[Dict[str, Any]], batch_idx: int) -> tuple:
        """ä¸ºæ‰¹æ¬¡æ•°æ®åˆ›å»ºCæ–‡ä»¶"""
        batch_dir = os.path.join(self.temp_dir, f"batch_{batch_idx}")
        os.makedirs(batch_dir, exist_ok=True)
        
        c_files = []
        
        for i, sample in enumerate(batch_data):
            func_code = sample['func']
            file_name = f"{batch_idx}_{i}.c"
            file_path = os.path.join(batch_dir, file_name)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(func_code)
            
            c_files.append(file_path)
        
        return batch_dir, c_files
    
    def batch_generate_cpg(self, c_files: List[str], batch_dir: str) -> List[str]:
        """æ‰¹é‡ç”ŸæˆCPGæ–‡ä»¶"""
        self.log(f"ğŸ”§ ç”ŸæˆCPGæ–‡ä»¶ ({len(c_files)}ä¸ªæ–‡ä»¶)...")
        
        cpg_dir = os.path.join(batch_dir, "cpg")
        os.makedirs(cpg_dir, exist_ok=True)
        
        cpg_files = []
        
        # é€ä¸ªç”ŸæˆCPG
        for c_file in c_files:
            file_name = os.path.basename(c_file).replace('.c', '')
            
            try:
                out_file = file_name + ".bin"
                cpg_output_path = os.path.join(cpg_dir, out_file)
                
                joern_parse_cmd = [
                    self.joern_parse,
                    c_file,
                    "--output", cpg_output_path
                ]
                
                result = subprocess.run(joern_parse_cmd, 
                                      stdout=subprocess.PIPE, 
                                      stderr=subprocess.PIPE, 
                                      text=True, 
                                      timeout=self.timeout_seconds,
                                      check=False)
                
                if os.path.exists(cpg_output_path) and os.path.getsize(cpg_output_path) > 100:
                    cpg_files.append(cpg_output_path)
                    
            except subprocess.TimeoutExpired:
                continue
            except Exception:
                continue
        
        self.log(f"  âœ… æˆåŠŸç”Ÿæˆ {len(cpg_files)}/{len(c_files)} ä¸ªCPGæ–‡ä»¶")
        return cpg_files
    
    def extract_json_from_cpg(self, cpg_files: List[str], batch_dir: str) -> List[str]:
        """ä»CPGæ–‡ä»¶æå–JSONæ•°æ® - ä½¿ç”¨VulLMGNNçš„æ–¹æ³•"""
        if not cpg_files:
            return []
        
        json_dir = os.path.join(batch_dir, "json")
        os.makedirs(json_dir, exist_ok=True)
        
        json_files = []
        
        # åˆ†å°æ‰¹æ¬¡å¤„ç†
        chunk_size = 5  # å‡å°‘æ‰¹æ¬¡å¤§å°é¿å…è¶…æ—¶
        
        for i in range(0, len(cpg_files), chunk_size):
            chunk_files = cpg_files[i:i + chunk_size]
            
            # å¯åŠ¨Joern CLI
            joern_cmd = [self.joern_cli]
            joern_process = subprocess.Popen(joern_cmd, 
                                           stdin=subprocess.PIPE, 
                                           stdout=subprocess.PIPE, 
                                           stderr=subprocess.PIPE,
                                           text=True)
            
            try:
                commands = []
                chunk_json_files = []
                
                for cpg_file in chunk_files:
                    file_name = os.path.basename(cpg_file).replace('.bin', '')
                    json_file_name = f"{file_name}.json"
                    chunk_json_files.append(json_file_name)
                    
                    json_output = os.path.join(json_dir, json_file_name)
                    
                    # ä½¿ç”¨VulLMGNNçš„Joernè„šæœ¬
                    commands.extend([
                        f'importCpg("{os.path.abspath(cpg_file)}")',
                        f'cpg.runScript("{self.joern_script}").toString() |> "{os.path.abspath(json_output)}"',
                        'delete'
                    ])
                
                commands.append('exit')
                command_script = '\n'.join(commands)
                
                timeout = max(30, len(chunk_files) * 8)
                outs, errs = joern_process.communicate(input=command_script, timeout=timeout)
                
                # æ£€æŸ¥ç”Ÿæˆçš„JSONæ–‡ä»¶
                for json_file in chunk_json_files:
                    json_path = os.path.join(json_dir, json_file)
                    if os.path.exists(json_path) and os.path.getsize(json_path) > 50:
                        json_files.append(json_path)
                
            except subprocess.TimeoutExpired:
                joern_process.kill()
                continue
            except Exception as e:
                joern_process.kill()
                self.log(f"âš ï¸ JSONæå–å¤±è´¥: {e}")
                continue
        
        self.log(f"  âœ… æˆåŠŸæå– {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        return json_files
    
    def process_cpg_json(self, json_files: List[str], batch_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†CPG JSONæ•°æ®"""
        cpg_results = []
        
        for json_file, sample in zip(json_files, batch_data[:len(json_files)]):
            try:
                # è¯»å–CPG JSONæ•°æ®
                with open(json_file, 'r', encoding='utf-8') as f:
                    cpg_data = json.load(f)
                
                if cpg_data and 'functions' in cpg_data and cpg_data['functions']:
                    # ä½¿ç”¨VulLMGNNçš„è§£æå™¨å¤„ç†CPGæ•°æ®
                    if VULLMGNN_UTILS_AVAILABLE:
                        try:
                            nodes = parse_complete_cpg_to_nodes(cpg_data)
                            if nodes:
                                cpg_result = {
                                    'cpg_data': cpg_data,
                                    'parsed_nodes': nodes,
                                    'target': sample['target'],
                                    'func': sample['func'],
                                    'metadata': sample['metadata'],
                                    'hash': sample['hash'],
                                    'index': sample['index'],
                                    'processing_method': 'vullmgnn_parser'
                                }
                                cpg_results.append(cpg_result)
                                continue
                        except Exception as e:
                            self.log(f"âš ï¸ VulLMGNNè§£æå¤±è´¥: {e}")
                    
                    # å›é€€åˆ°ç®€å•å¤„ç†
                    cpg_result = {
                        'cpg_data': cpg_data,
                        'target': sample['target'],
                        'func': sample['func'],
                        'metadata': sample['metadata'],
                        'hash': sample['hash'],
                        'index': sample['index'],
                        'processing_method': 'simple'
                    }
                    cpg_results.append(cpg_result)
                
            except Exception as e:
                self.log(f"âš ï¸ JSONå¤„ç†å¤±è´¥: {e}")
                continue
        
        return cpg_results
    
    def process_batch(self, batch_data: List[Dict[str, Any]], batch_idx: int) -> List[Dict[str, Any]]:
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
        self.log(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1} ({len(batch_data)}ä¸ªæ ·æœ¬)")
        
        try:
            # æ­¥éª¤1: åˆ›å»ºCæ–‡ä»¶
            batch_dir, c_files = self.create_batch_c_files(batch_data, batch_idx)
            
            # æ­¥éª¤2: ç”ŸæˆCPG
            cpg_files = self.batch_generate_cpg(c_files, batch_dir)
            
            # æ­¥éª¤3: æå–JSON
            json_files = self.extract_json_from_cpg(cpg_files, batch_dir)
            
            # æ­¥éª¤4: å¤„ç†CPGæ•°æ®
            cpg_results = self.process_cpg_json(json_files, batch_data)
            
            return cpg_results
            
        except Exception as e:
            self.log(f"  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            return []
    
    def save_cpg_data(self, cpg_results: List[Dict[str, Any]]):
        """ä¿å­˜CPGæ•°æ®"""
        self.log("ğŸ’¾ ä¿å­˜CPGæ•°æ®...")

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        input_basename = os.path.basename(self.input_json)
        input_name_without_ext = os.path.splitext(input_basename)[0]
        output_filename = f"fixed_cpg_processed_{input_name_without_ext}.json"
        output_file = os.path.join(self.cpg_data_dir, output_filename)

        # åºåˆ—åŒ–CPGæ•°æ®ï¼ˆå¤„ç†CompleteNodeå¯¹è±¡ï¼‰
        serializable_results = self._make_serializable(cpg_results)

        # ä¿å­˜æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        self.log(f"âœ… CPGæ•°æ®å·²ä¿å­˜: {len(serializable_results)} ä¸ªæ ·æœ¬ -> {output_file}")
        return output_file

    def _make_serializable(self, data):
        """å°†æ•°æ®è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(data, dict):
            return {k: self._make_serializable(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._make_serializable(item) for item in data]
        elif hasattr(data, '__dict__'):
            # å¤„ç†CompleteNodeç­‰å¯¹è±¡
            if hasattr(data, 'id') and hasattr(data, 'label'):
                # è¿™æ˜¯ä¸€ä¸ªèŠ‚ç‚¹å¯¹è±¡
                properties = getattr(data, 'properties', {})
                # é€’å½’å¤„ç†properties
                serializable_properties = self._make_serializable(properties)

                return {
                    'id': str(getattr(data, 'id', '')),
                    'label': str(getattr(data, 'label', '')),
                    'code': str(getattr(data, 'code', '')),
                    'type': str(getattr(data, 'type', '')),
                    'properties': serializable_properties
                }
            else:
                # å…¶ä»–å¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºå­—å…¸
                try:
                    result = {}
                    for k, v in data.__dict__.items():
                        if not k.startswith('_'):
                            try:
                                result[k] = self._make_serializable(v)
                            except:
                                result[k] = str(v)
                    return result
                except:
                    return str(data)
        elif hasattr(data, '__iter__') and not isinstance(data, (str, bytes)):
            # å¤„ç†å…¶ä»–å¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚Propertiesï¼‰
            try:
                if hasattr(data, 'items'):
                    # ç±»ä¼¼å­—å…¸çš„å¯¹è±¡
                    return {str(k): self._make_serializable(v) for k, v in data.items()}
                else:
                    # ç±»ä¼¼åˆ—è¡¨çš„å¯¹è±¡
                    return [self._make_serializable(item) for item in data]
            except:
                return str(data)
        else:
            # åŸºæœ¬ç±»å‹æˆ–æ— æ³•å¤„ç†çš„å¯¹è±¡
            try:
                # å°è¯•ç›´æ¥åºåˆ—åŒ–
                json.dumps(data)
                return data
            except:
                return str(data)
    
    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
            self.log("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
    
    def run_processing(self):
        """è¿è¡Œå®Œæ•´å¤„ç†æµç¨‹"""
        start_time = time.time()
        
        try:
            self.log("ğŸ¯ å¼€å§‹ä¿®å¤ç‰ˆJoern CPGå¤„ç†...")
            
            # æ­¥éª¤1: åŠ è½½å’Œæ¸…ç†æ•°æ®
            cleaned_data = self.load_and_clean_data()
            
            if not cleaned_data:
                raise Exception("æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
            
            # æ­¥éª¤2: æ‰¹é‡å¤„ç†
            all_cpg_results = []
            total_samples = len(cleaned_data)
            
            for i in range(0, total_samples, self.batch_size):
                batch_end = min(i + self.batch_size, total_samples)
                batch_data = cleaned_data[i:batch_end]
                batch_idx = i // self.batch_size
                
                batch_start_time = time.time()
                batch_results = self.process_batch(batch_data, batch_idx)
                batch_time = time.time() - batch_start_time
                
                all_cpg_results.extend(batch_results)
                
                # æ˜¾ç¤ºè¿›åº¦
                success_rate = len(batch_results) / len(batch_data) * 100
                speed = len(batch_results) / batch_time if batch_time > 0 else 0
                
                self.log(f"  âœ… æ‰¹æ¬¡å®Œæˆ: {len(batch_results)}/{len(batch_data)} æˆåŠŸ ({success_rate:.1f}%), é€Ÿåº¦: {speed:.1f}æ ·æœ¬/ç§’")
            
            if not all_cpg_results:
                raise Exception("æ²¡æœ‰æˆåŠŸå¤„ç†çš„CPGæ•°æ®")
            
            # æ­¥éª¤3: ä¿å­˜æ•°æ®
            output_file = self.save_cpg_data(all_cpg_results)
            
            # æ­¥éª¤4: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self.cleanup_temp_files()
            
            total_time = time.time() - start_time
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            self.log("ğŸ‰ ä¿®å¤ç‰ˆJoern CPGå¤„ç†å®Œæˆï¼")
            self.log(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            self.log(f"ğŸ“„ CPGæ•°æ®æ–‡ä»¶: {output_file}")
            self.log(f"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
            self.log(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(all_cpg_results)/total_time:.2f}æ ·æœ¬/ç§’")
            
            success_rate = len(all_cpg_results) / len(cleaned_data) * 100
            self.log(f"âœ… æ€»è®¡å¤„ç†: {len(all_cpg_results)}/{len(cleaned_data)} æ ·æœ¬ ({success_rate:.1f}%æˆåŠŸç‡)")
            
            return True
            
        except Exception as e:
            self.log(f"âŒ ä¿®å¤ç‰ˆJoern CPGå¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¿®å¤ç‰ˆæ­¥éª¤1: Joern CPGå¤„ç†")
    
    parser.add_argument("--input", default="../output/vuln_types/all_vul_train.json", 
                       help="è¾“å…¥çš„å¤šåˆ†ç±»JSONæ–‡ä»¶")
    parser.add_argument("--output", default="./step1_fixed_output/", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--joern_path", default="/root/autodl-tmp/vullmgnn/joern/joern-cli/", 
                       help="Joernå·¥å…·è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=20, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--timeout", type=int, default=15, help="è¶…æ—¶æ—¶é—´(ç§’)")
    parser.add_argument("--max_samples", type=int, help="æœ€å¤§å¤„ç†æ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return 1
    
    # é™åˆ¶æ ·æœ¬æ•°
    if args.max_samples:
        with open(args.input, 'r') as f:
            data = json.load(f)
        
        limited_data = data[:args.max_samples]
        limited_file = args.input.replace('.json', f'_limited_{args.max_samples}.json')
        
        with open(limited_file, 'w') as f:
            json.dump(limited_data, f, indent=2, ensure_ascii=False)
        
        args.input = limited_file
        print(f"ğŸ“Š é™åˆ¶æ ·æœ¬æ•°: {args.max_samples}")
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = FixedJoernProcessor(
        input_json=args.input,
        output_dir=args.output,
        joern_path=args.joern_path,
        batch_size=args.batch_size,
        timeout_seconds=args.timeout,
        verbose=True
    )
    
    success = processor.run_processing()
    
    if success:
        print(f"\nğŸ¯ ä¿®å¤ç‰ˆæ­¥éª¤1å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡º: {args.output}")
        print(f"â¡ï¸ ä¸‹ä¸€æ­¥: è¿è¡Œæ­¥éª¤2å’Œæ­¥éª¤3")
        return 0
    else:
        print(f"\nâŒ ä¿®å¤ç‰ˆæ­¥éª¤1å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())
