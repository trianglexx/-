#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‰CWEç±»å‹åˆ›å»ºRevealæ ¼å¼äºŒåˆ†ç±»æ•°æ®é›†
æ¯ä¸ªCWEç±»å‹å•ç‹¬å»ºæ–‡ä»¶å¤¹ï¼ŒåŒ…å«å¯¹åº”çš„Revealæ ¼å¼æ•°æ®
æ‰€æœ‰æ ‡ç­¾è®¾ä¸º1ï¼ˆå› ä¸ºéƒ½æ˜¯æ¼æ´æ ·æœ¬ï¼‰
"""

import json
import numpy as np
import os
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm
import logging
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] INFO: %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)


class CWERevealDatasetCreator:
    """æŒ‰CWEç±»å‹åˆ›å»ºRevealæ ¼å¼æ•°æ®é›†"""
    
    def __init__(self, input_dir: str = "livable_multiclass_data", output_base: str = "cwe_reveal_datasets"):
        self.input_dir = Path(input_dir)
        self.output_base = Path(output_base)
        self.output_base.mkdir(exist_ok=True)
        
        # è¾¹ç±»å‹æ˜ å°„
        self.edge_type_names = ["AST", "CFG", "DFG", "CDG"]
        
        logger.info(f"ğŸ”„ CWEåˆ†ç±»Revealæ•°æ®é›†åˆ›å»ºå™¨åˆå§‹åŒ–")
        logger.info(f"ğŸ“‚ è¾“å…¥ç›®å½•: {self.input_dir}")
        logger.info(f"ğŸ“‚ è¾“å‡ºæ ¹ç›®å½•: {self.output_base}")
    
    def convert_structure_to_edges(self, structure: List[List[int]]) -> List[Dict]:
        """å°†structureä¿¡æ¯è½¬æ¢ä¸ºè¾¹åˆ—è¡¨"""
        edges = []
        
        for edge_info in structure:
            if len(edge_info) >= 3:
                source, target, edge_type = edge_info[0], edge_info[1], edge_info[2]
                
                # ç¡®ä¿è¾¹ç±»å‹æœ‰æ•ˆ
                if 0 <= edge_type < len(self.edge_type_names):
                    type_name = self.edge_type_names[edge_type]
                else:
                    type_name = "AST"  # é»˜è®¤ç±»å‹
                
                edges.append({
                    "source": source,
                    "target": target,
                    "type": type_name
                })
        
        return edges
    
    def create_reveal_nodes(self, features: List[List[float]]) -> List[Dict]:
        """åˆ›å»ºRevealæ ¼å¼çš„èŠ‚ç‚¹åˆ—è¡¨"""
        nodes = []
        for i, node_features in enumerate(features):
            nodes.append({
                "id": i,
                "features": node_features,  # ä¿æŒ768ç»´
                "type": "CODE_NODE"
            })
        return nodes
    
    def convert_sample_to_reveal(self, sample: Dict[str, Any], sample_id: int, cwe_type: str) -> Dict[str, Any]:
        """è½¬æ¢å•ä¸ªæ ·æœ¬ä¸ºRevealæ ¼å¼ï¼ˆäºŒåˆ†ç±»ï¼Œæ ‡ç­¾=1ï¼‰"""
        # æå–æ•°æ®
        features = sample["features"]
        structure = sample["structure"]
        metadata = sample.get("metadata", {})
        
        # è½¬æ¢ä¸ºRevealæ ¼å¼
        nodes = self.create_reveal_nodes(features)
        edges = self.convert_structure_to_edges(structure)
        
        # åˆ›å»ºRevealæ ·æœ¬ï¼ˆäºŒåˆ†ç±»ï¼Œæ ‡ç­¾=1è¡¨ç¤ºæ¼æ´ï¼‰
        reveal_sample = {
            "node_features": [node["features"] for node in nodes],
            "graph": [nodes, edges],
            "targets": [1],  # äºŒåˆ†ç±»æ ‡ç­¾ï¼Œ1=æ¼æ´
            "metadata": {
                "sample_id": sample_id,
                "cwe_type": cwe_type,
                "num_nodes": len(nodes),
                "num_edges": len(edges),
                "feature_dim": 768,
                "binary_classification": True,
                "label_meaning": "1=vulnerability, 0=normal",
                **metadata
            }
        }
        
        return reveal_sample
    
    def collect_cwe_data(self) -> Dict[str, Dict[str, List]]:
        """æ”¶é›†æŒ‰CWEç±»å‹åˆ†ç»„çš„æ•°æ®"""
        logger.info("ğŸ“‹ æ”¶é›†CWEåˆ†ç±»æ•°æ®...")
        
        cwe_data = defaultdict(lambda: {"train": [], "valid": [], "test": []})
        
        for split in ["train", "valid", "test"]:
            input_file = self.input_dir / f"livable_{split}.json"
            if not input_file.exists():
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
                continue
            
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"ğŸ“Š å¤„ç† {split} æ•°æ®é›†: {len(data)} æ ·æœ¬")
            
            for sample in data:
                cwe_id = sample['metadata']['cwe_id']
                cwe_data[cwe_id][split].append(sample)
        
        logger.info(f"âœ… æ”¶é›†å®Œæˆ: {len(cwe_data)} ä¸ªCWEç±»å‹")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        for cwe_id, splits in cwe_data.items():
            total = sum(len(splits[split]) for split in ["train", "valid", "test"])
            logger.info(f"  {cwe_id}: {total}ä¸ªæ ·æœ¬ (è®­ç»ƒ:{len(splits['train'])}, éªŒè¯:{len(splits['valid'])}, æµ‹è¯•:{len(splits['test'])})")
        
        return dict(cwe_data)
    
    def create_cwe_dataset(self, cwe_id: str, cwe_data: Dict[str, List]) -> bool:
        """ä¸ºå•ä¸ªCWEç±»å‹åˆ›å»ºRevealæ ¼å¼æ•°æ®é›†"""
        logger.info(f"ğŸ”„ åˆ›å»º {cwe_id} æ•°æ®é›†...")
        
        # åˆ›å»ºCWEä¸“ç”¨æ–‡ä»¶å¤¹
        cwe_dir = self.output_base / cwe_id.lower()
        cwe_dir.mkdir(exist_ok=True)
        
        total_samples = 0
        
        for split in ["train", "valid", "test"]:
            if not cwe_data[split]:
                logger.warning(f"âš ï¸ {cwe_id} {split} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            logger.info(f"  å¤„ç† {split} æ•°æ®: {len(cwe_data[split])} æ ·æœ¬")
            
            # è½¬æ¢æ ·æœ¬
            reveal_samples = []
            for i, sample in enumerate(tqdm(cwe_data[split], desc=f"è½¬æ¢{cwe_id}-{split}")):
                try:
                    reveal_sample = self.convert_sample_to_reveal(sample, i, cwe_id)
                    reveal_samples.append(reveal_sample)
                except Exception as e:
                    logger.warning(f"âš ï¸ {cwe_id} {split} æ ·æœ¬ {i} è½¬æ¢å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜æ•°æ®
            if reveal_samples:
                output_file = cwe_dir / f"reveal-{split}-v2.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(reveal_samples, f, ensure_ascii=False, indent=2)
                
                total_samples += len(reveal_samples)
                logger.info(f"  âœ… {split}: {len(reveal_samples)} æ ·æœ¬ -> {output_file}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        self.create_cwe_config(cwe_dir, cwe_id, total_samples)
        
        logger.info(f"ğŸ¯ {cwe_id} æ•°æ®é›†åˆ›å»ºå®Œæˆ: {total_samples} ä¸ªæ ·æœ¬")
        return True
    
    def create_cwe_config(self, cwe_dir: Path, cwe_id: str, total_samples: int):
        """åˆ›å»ºCWEæ•°æ®é›†é…ç½®æ–‡ä»¶"""
        config = {
            "cwe_type": cwe_id,
            "format": "reveal_binary_classification",
            "feature_dim": 768,
            "num_classes": 2,
            "class_labels": {
                "0": "normal",
                "1": "vulnerability"
            },
            "current_data": {
                "label": 1,
                "description": f"All samples are {cwe_id} vulnerabilities"
            },
            "graph_types": ["AST", "CFG", "DFG", "CDG"],
            "splits": {
                "train": "reveal-train-v2.json",
                "valid": "reveal-valid-v2.json",
                "test": "reveal-test-v2.json"
            },
            "total_samples": total_samples,
            "converted_from": "livable_multiclass_data",
            "description": f"Binary classification dataset for {cwe_id} vulnerability type. All samples have label=1 (vulnerability). To create a complete binary dataset, add normal samples with label=0.",
            "note": "This dataset contains only vulnerability samples. For true binary classification, normal/benign samples with label=0 should be added."
        }
        
        config_file = cwe_dir / "dataset_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_file}")
    
    def create_all_cwe_datasets(self) -> bool:
        """åˆ›å»ºæ‰€æœ‰CWEç±»å‹çš„æ•°æ®é›†"""
        logger.info("ğŸš€ å¼€å§‹åˆ›å»ºæ‰€æœ‰CWEç±»å‹çš„Revealæ•°æ®é›†...")
        
        # æ”¶é›†CWEåˆ†ç±»æ•°æ®
        cwe_data = self.collect_cwe_data()
        
        success_count = 0
        
        # ä¸ºæ¯ä¸ªCWEç±»å‹åˆ›å»ºæ•°æ®é›†
        for cwe_id, splits_data in cwe_data.items():
            try:
                if self.create_cwe_dataset(cwe_id, splits_data):
                    success_count += 1
            except Exception as e:
                logger.error(f"âŒ {cwe_id} æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
                continue
        
        # åˆ›å»ºæ€»ä½“è¯´æ˜æ–‡ä»¶
        self.create_master_readme(cwe_data)
        
        logger.info(f"ğŸ‰ å®Œæˆ! æˆåŠŸåˆ›å»º {success_count}/{len(cwe_data)} ä¸ªCWEæ•°æ®é›†")
        return success_count == len(cwe_data)
    
    def create_master_readme(self, cwe_data: Dict[str, Dict]):
        """åˆ›å»ºæ€»ä½“è¯´æ˜æ–‡ä»¶"""
        readme_content = f"""# CWEåˆ†ç±»Revealæ ¼å¼æ•°æ®é›†

## ğŸ“Š æ•°æ®é›†æ¦‚è¿°

æœ¬æ•°æ®é›†å°†åŸå§‹çš„å¤šåˆ†ç±»æ¼æ´æ•°æ®æŒ‰CWEç±»å‹åˆ†ç±»ï¼Œæ¯ä¸ªCWEç±»å‹åŒ…å«ç‹¬ç«‹çš„Revealæ ¼å¼äºŒåˆ†ç±»æ•°æ®é›†ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
cwe_reveal_datasets/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ dataset_summary.json         # æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
"""

        total_samples = 0
        for cwe_id, splits_data in sorted(cwe_data.items()):
            cwe_total = sum(len(splits_data[split]) for split in ["train", "valid", "test"])
            total_samples += cwe_total
            
            readme_content += f"â”œâ”€â”€ {cwe_id.lower()}/                    # {cwe_id} æ¼æ´ç±»å‹ ({cwe_total}ä¸ªæ ·æœ¬)\n"
            readme_content += f"â”‚   â”œâ”€â”€ reveal-train-v2.json      # è®­ç»ƒé›† ({len(splits_data['train'])}ä¸ª)\n"
            readme_content += f"â”‚   â”œâ”€â”€ reveal-valid-v2.json      # éªŒè¯é›† ({len(splits_data['valid'])}ä¸ª)\n"
            readme_content += f"â”‚   â”œâ”€â”€ reveal-test-v2.json       # æµ‹è¯•é›† ({len(splits_data['test'])}ä¸ª)\n"
            readme_content += f"â”‚   â””â”€â”€ dataset_config.json       # é…ç½®æ–‡ä»¶\n"

        readme_content += f"""```

## ğŸ¯ æ•°æ®ç‰¹ç‚¹

- **äºŒåˆ†ç±»æ ‡ç­¾**: æ‰€æœ‰æ ·æœ¬æ ‡ç­¾ä¸º1ï¼ˆæ¼æ´ï¼‰ï¼Œéœ€è¦æ·»åŠ æ ‡ç­¾ä¸º0çš„æ­£å¸¸æ ·æœ¬æ¥å®ŒæˆäºŒåˆ†ç±»
- **768ç»´ç‰¹å¾**: ä¿æŒGraphCodeBERTåŸå§‹åµŒå…¥ç»´åº¦
- **å›¾ç»“æ„**: åŒ…å«AST, CFG, DFG, CDGå››ç§è¾¹ç±»å‹
- **æ€»æ ·æœ¬æ•°**: {total_samples:,}ä¸ªæ¼æ´æ ·æœ¬
- **CWEç±»å‹æ•°**: {len(cwe_data)}ç§

## ğŸ“ˆ CWEç±»å‹åˆ†å¸ƒ

| CWEç±»å‹ | è®­ç»ƒé›† | éªŒè¯é›† | æµ‹è¯•é›† | æ€»è®¡ | æè¿° |
|---------|--------|--------|--------|------|------|
"""

        cwe_descriptions = {
            "CWE-119": "ç¼“å†²åŒºæº¢å‡º",
            "CWE-20": "è¾“å…¥éªŒè¯ä¸å½“",
            "CWE-399": "èµ„æºç®¡ç†é”™è¯¯",
            "CWE-125": "è¶Šç•Œè¯»å–",
            "CWE-264": "æƒé™å’Œè®¿é—®æ§åˆ¶",
            "CWE-200": "ä¿¡æ¯æ³„éœ²",
            "CWE-189": "æ•°å€¼é”™è¯¯",
            "CWE-416": "é‡Šæ”¾åä½¿ç”¨",
            "CWE-190": "æ•´æ•°æº¢å‡º",
            "CWE-362": "ç«æ€æ¡ä»¶",
            "CWE-476": "ç©ºæŒ‡é’ˆè§£å¼•ç”¨",
            "CWE-787": "è¶Šç•Œå†™å…¥",
            "CWE-284": "è®¿é—®æ§åˆ¶ä¸å½“",
            "CWE-254": "å®‰å…¨ç‰¹æ€§"
        }

        for cwe_id, splits_data in sorted(cwe_data.items(), key=lambda x: sum(len(x[1][s]) for s in ["train","valid","test"]), reverse=True):
            train_count = len(splits_data['train'])
            valid_count = len(splits_data['valid'])
            test_count = len(splits_data['test'])
            total_count = train_count + valid_count + test_count
            desc = cwe_descriptions.get(cwe_id, "")
            
            readme_content += f"| {cwe_id} | {train_count:,} | {valid_count:,} | {test_count:,} | {total_count:,} | {desc} |\n"

        readme_content += f"""
## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŠ è½½å•ä¸ªCWEæ•°æ®é›†
```python
import json

# åŠ è½½CWE-119ç¼“å†²åŒºæº¢å‡ºæ•°æ®
with open('cwe_reveal_datasets/cwe-119/reveal-train-v2.json', 'r') as f:
    cwe119_train = json.load(f)

print(f"CWE-119è®­ç»ƒæ ·æœ¬æ•°: {{len(cwe119_train)}}")
print(f"æ ·æœ¬æ ¼å¼: {{list(cwe119_train[0].keys())}}")
print(f"æ ‡ç­¾: {{cwe119_train[0]['targets']}}")  # åº”è¯¥æ˜¯[1]
```

### åˆ›å»ºå®Œæ•´äºŒåˆ†ç±»æ•°æ®é›†
```python
# æ³¨æ„ï¼šå½“å‰æ‰€æœ‰æ ·æœ¬æ ‡ç­¾éƒ½æ˜¯1ï¼ˆæ¼æ´ï¼‰
# éœ€è¦æ·»åŠ æ ‡ç­¾ä¸º0çš„æ­£å¸¸æ ·æœ¬æ¥åˆ›å»ºå®Œæ•´çš„äºŒåˆ†ç±»æ•°æ®é›†

# ç¤ºä¾‹ï¼šåˆå¹¶æ­£å¸¸æ ·æœ¬
normal_samples = load_normal_samples()  # éœ€è¦è‡ªå·±æä¾›
for sample in normal_samples:
    sample['targets'] = [0]  # è®¾ç½®æ­£å¸¸æ ·æœ¬æ ‡ç­¾

# åˆå¹¶æ¼æ´æ ·æœ¬å’Œæ­£å¸¸æ ·æœ¬
complete_dataset = cwe119_train + normal_samples
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ ‡ç­¾å«ä¹‰**: 
   - 1 = æ¼æ´æ ·æœ¬
   - 0 = æ­£å¸¸æ ·æœ¬ï¼ˆéœ€è¦è‡ªè¡Œæ·»åŠ ï¼‰

2. **æ•°æ®å®Œæ•´æ€§**: 
   - å½“å‰åªåŒ…å«æ¼æ´æ ·æœ¬
   - è¦è¿›è¡ŒçœŸæ­£çš„äºŒåˆ†ç±»ï¼Œéœ€è¦æ·»åŠ æ­£å¸¸ä»£ç æ ·æœ¬

3. **ç‰¹å¾æ ¼å¼**:
   - `node_features`: 768ç»´ç‰¹å¾æ•°ç»„
   - `graph`: [nodes, edges] å›¾ç»“æ„
   - `targets`: [1] äºŒåˆ†ç±»æ ‡ç­¾

## ğŸ¯ åº”ç”¨åœºæ™¯

- ç‰¹å®šCWEç±»å‹çš„æ¼æ´æ£€æµ‹
- äºŒåˆ†ç±»æ¼æ´æ£€æµ‹æ¨¡å‹è®­ç»ƒ
- å›¾ç¥ç»ç½‘ç»œä»£ç åˆ†æ
- å®‰å…¨ç ”ç©¶å’ŒåŸºå‡†æµ‹è¯•

ç”Ÿæˆæ—¶é—´: $(date)
"""

        # ä¿å­˜README
        readme_file = self.output_base / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        summary = {
            "total_samples": total_samples,
            "num_cwe_types": len(cwe_data),
            "cwe_distribution": {
                cwe_id: {
                    "train": len(splits_data['train']),
                    "valid": len(splits_data['valid']),
                    "test": len(splits_data['test']),
                    "total": sum(len(splits_data[s]) for s in ["train", "valid", "test"])
                }
                for cwe_id, splits_data in cwe_data.items()
            },
            "format": "reveal_binary_classification",
            "feature_dim": 768,
            "all_labels": 1,
            "note": "All samples are vulnerability samples with label=1. Normal samples with label=0 need to be added for complete binary classification."
        }
        
        summary_file = self.output_base / "dataset_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ æ€»ä½“è¯´æ˜: {readme_file}")
        logger.info(f"ğŸ“Š ç»Ÿè®¡æ‘˜è¦: {summary_file}")


def main():
    """ä¸»å‡½æ•°"""
    creator = CWERevealDatasetCreator()
    
    success = creator.create_all_cwe_datasets()
    
    if success:
        print(f"\nğŸ‰ æ‰€æœ‰CWEæ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: cwe_reveal_datasets/")
        print(f"\nğŸ“Š åˆ›å»ºçš„æ•°æ®é›†:")
        print(f"  - 14ä¸ªCWEç±»å‹çš„ç‹¬ç«‹æ•°æ®é›†")
        print(f"  - æ¯ä¸ªåŒ…å«train/valid/testä¸‰ä¸ªsplit")
        print(f"  - æ‰€æœ‰æ ·æœ¬æ ‡ç­¾ä¸º1ï¼ˆæ¼æ´ï¼‰")
        print(f"  - 768ç»´GraphCodeBERTç‰¹å¾")
        print(f"  - æ ‡å‡†Revealæ ¼å¼")
        print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print(f"  - æŸ¥çœ‹ cwe_reveal_datasets/README.md äº†è§£è¯¦æƒ…")
        print(f"  - éœ€è¦æ·»åŠ æ ‡ç­¾ä¸º0çš„æ­£å¸¸æ ·æœ¬æ¥å®ŒæˆäºŒåˆ†ç±»")
        print(f"  - æ¯ä¸ªCWEæ–‡ä»¶å¤¹åŒ…å«ç‹¬ç«‹çš„æ•°æ®é›†")
        return 0
    else:
        print(f"\nâŒ éƒ¨åˆ†æ•°æ®é›†åˆ›å»ºå¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())