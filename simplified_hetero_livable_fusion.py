#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆå¼‚æ„GNN + LIVABLEèåˆæ¶æ„
ä¸“æ³¨äºæ ¸å¿ƒç®—æ³•èåˆï¼Œç®€åŒ–æ•°æ®å¤„ç†
"""

import json
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
import logging
from pathlib import Path
from tqdm import tqdm
import copy

# å¯¼å…¥ç°æœ‰çš„å¼‚æ„GNNç»„ä»¶ï¼Œä½†ä¸æ‰§è¡Œå…¶è®­ç»ƒä»£ç 
import sys
import importlib.util
spec = importlib.util.spec_from_file_location("hetero_gnn", "heterogeneous_gnn_pyg.py")
hetero_gnn = importlib.util.module_from_spec(spec)
sys.modules["hetero_gnn"] = hetero_gnn

# åªå¯¼å…¥éœ€è¦çš„ç±»ï¼Œä¸æ‰§è¡Œè®­ç»ƒä»£ç 
with open('heterogeneous_gnn_pyg.py', 'r') as f:
    source_code = f.read()
    
# åªæ‰§è¡Œç±»å®šä¹‰å’Œå‡½æ•°å®šä¹‰ï¼Œè·³è¿‡ if __name__ == '__main__' éƒ¨åˆ†
lines = source_code.split('\n')
filtered_lines = []
skip_main = False
for line in lines:
    if line.strip().startswith("if __name__ == '__main__':"):
        skip_main = True
        continue
    if not skip_main:
        filtered_lines.append(line)

filtered_code = '\n'.join(filtered_lines)
exec(filtered_code)

# å¯¼å…¥å¿…è¦çš„PyGç»„ä»¶
from torch_geometric.data import Dataset, Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import MessagePassing, global_mean_pool, global_max_pool

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] INFO: %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger(__name__)


# ç›´æ¥ä½¿ç”¨ç°æœ‰çš„HeterogeneousLIVABLEPygModelï¼Œä¸éœ€è¦é‡æ–°å®šä¹‰


def train_livable_enhanced_hetero_gnn():
    """è®­ç»ƒLIVABLEå¢å¼ºçš„å¼‚æ„GNN"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ® - åªä½¿ç”¨è®­ç»ƒé›†
    logger.info("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
    with open('livable_multiclass_data/livable_train.json', 'r') as f:
        train_data = json.load(f)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ - åªæœ‰è®­ç»ƒé›†
    train_dataset = HeterogeneousPygDataset(root=None, data_list=train_data, max_seq_len=6)
    
    from torch_geometric.loader import DataLoader
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    
    # ä½¿ç”¨ç°æœ‰çš„å¼‚æ„GNNæ¨¡å‹
    model = HeterogeneousLIVABLEPygModel(
        input_dim=768,
        hidden_dim=256,
        num_classes=14,
        num_gnn_layers=3,
        num_edge_types=4,
        dropout=0.2,
        alpha=0.1
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=5)
    
    # è®­ç»ƒå‚æ•° - ä¸“æ³¨è®­ç»ƒé›†å­¦ä¹ åˆ†æ
    num_epochs = 53
    best_train_f1 = 0.0
    best_model = None
    
    logger.info("ğŸš€ å¼€å§‹LIVABLEå¢å¼ºå¼‚æ„GNNè®­ç»ƒé›†å­¦ä¹ åˆ†æ (50è½®)")
    
    training_history = {
        'train_loss': [],
        'train_accuracy': [],
        'train_f1': [],
        'class_accuracies_history': []
    }
    
    # CWEç±»åˆ«åç§°
    cwe_names = ['CWE-119', 'CWE-20', 'CWE-399', 'CWE-125', 'CWE-264', 'CWE-200', 
                 'CWE-189', 'CWE-416', 'CWE-190', 'CWE-362', 'CWE-476', 'CWE-787', 
                 'CWE-284', 'CWE-254']
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            batch = batch.to(device)
            
            optimizer.zero_grad()
            
            outputs = model(batch)
            loss = criterion(outputs, batch.y)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(batch.y.cpu().numpy())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss = total_loss / len(train_loader)
        train_accuracy = accuracy_score(all_labels, all_preds)
        _, _, train_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = {}
        for i in range(14):
            mask = np.array(all_labels) == i
            if mask.sum() > 0:
                class_acc = (np.array(all_preds)[mask] == i).sum() / mask.sum()
                class_accuracies[cwe_names[i]] = class_acc
            else:
                class_accuracies[cwe_names[i]] = 0.0
        
        # è®°å½•å†å²
        training_history['train_loss'].append(train_loss)
        training_history['train_accuracy'].append(train_accuracy)
        training_history['train_f1'].append(train_f1)
        training_history['class_accuracies_history'].append(class_accuracies.copy())
        
        logger.info(f"Epoch {epoch+1}: Loss={train_loss:.4f}, Train Acc={train_accuracy:.4f}, Train F1={train_f1:.4f}")
        
        # ä¿å­˜æœ€ä½³è®­ç»ƒæ¨¡å‹
        if train_f1 > best_train_f1:
            best_train_f1 = train_f1
            best_model = copy.deepcopy(model)
            logger.info(f"ğŸ¯ æ–°çš„æœ€ä½³è®­ç»ƒF1: {best_train_f1:.4f}")
        
        # å­¦ä¹ ç‡è°ƒæ•´ï¼ˆåŸºäºè®­ç»ƒF1ï¼‰
        scheduler.step(train_f1)
    
    # æœ€ç»ˆè®­ç»ƒé›†åˆ†æ
    logger.info("ğŸ“Š åˆ†æè®­ç»ƒé›†å­¦ä¹ æ•ˆæœ...")
    
    # è·å–è¾¹ç±»å‹é‡è¦æ€§ 
    edge_importance = best_model.get_edge_type_importance().cpu().detach().numpy().tolist()
    
    # åˆ†æç±»åˆ«å‡†ç¡®ç‡å˜åŒ–
    logger.info("ğŸ¯ å„ç±»åˆ«è®­ç»ƒå‡†ç¡®ç‡å˜åŒ–åˆ†æ:")
    logger.info("=" * 80)
    
    first_epoch = training_history['class_accuracies_history'][0]
    last_epoch = training_history['class_accuracies_history'][-1]
    
    logger.info(f"{'CWEç±»åˆ«':<12} | {'åˆå§‹å‡†ç¡®ç‡':<12} | {'æœ€ç»ˆå‡†ç¡®ç‡':<12} | {'æå‡å¹…åº¦':<12} | {'ç›¸å¯¹æå‡':<12}")
    logger.info("-" * 80)
    
    for cwe in cwe_names:
        initial_acc = first_epoch[cwe]
        final_acc = last_epoch[cwe]
        improvement = final_acc - initial_acc
        relative_improvement = (improvement / (initial_acc + 1e-8)) * 100 if initial_acc > 0 else float('inf') if improvement > 0 else 0
        
        logger.info(f"{cwe:<12} | {initial_acc:>11.4f} | {final_acc:>11.4f} | {improvement:>+11.4f} | {relative_improvement:>+10.1f}%")
    
    # è®¡ç®—æœ€ç»ˆè®­ç»ƒé›†å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    logger.info("\nğŸ“‹ æœ€ç»ˆè®­ç»ƒé›†å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
    logger.info("=" * 80)
    
    # ä½¿ç”¨æœ€åä¸€è½®çš„é¢„æµ‹ç»“æœ
    final_precision, final_recall, final_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)
    final_classification_report = classification_report(all_labels, all_preds, target_names=cwe_names, output_dict=True)
    
    logger.info(f"{'CWEç±»åˆ«':<12} | {'å‡†ç¡®ç‡':<10} | {'ç²¾ç¡®ç‡':<10} | {'å¬å›ç‡':<10} | {'F1åˆ†æ•°':<10}")
    logger.info("-" * 70)
    
    for i, cwe in enumerate(cwe_names):
        accuracy = last_epoch[cwe]
        precision = final_precision[i] if i < len(final_precision) else 0.0
        recall = final_recall[i] if i < len(final_recall) else 0.0
        f1 = final_f1[i] if i < len(final_f1) else 0.0
        
        logger.info(f"{cwe:<12} | {accuracy:>9.4f} | {precision:>9.4f} | {recall:>9.4f} | {f1:>9.4f}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'model_type': 'LIVABLE_Enhanced_HeterogeneousGNN_Training_Analysis',
        'final_train_f1': best_train_f1,
        'final_train_accuracy': training_history['train_accuracy'][-1],
        'final_train_loss': training_history['train_loss'][-1],
        'edge_type_importance': edge_importance,
        'model_parameters': sum(p.numel() for p in best_model.parameters()),
        'livable_parameters': {
            'appnp_k': 16,
            'appnp_alpha': 0.1
        },
        'training_history': training_history,
        'class_accuracy_analysis': {
            'initial': first_epoch,
            'final': last_epoch,
            'improvements': {cwe: last_epoch[cwe] - first_epoch[cwe] for cwe in cwe_names}
        },
        'final_classification_report': final_classification_report
    }
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = Path('livable_enhanced_hetero_training_analysis')
    results_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜ç»“æœ
    with open(results_dir / 'results.json', 'w') as f:
        json.dump(results, f, indent=2, default=float)
    
    torch.save(best_model.state_dict(), results_dir / 'best_model.pth')
    
    logger.info("ğŸ‰ LIVABLEå¢å¼ºå¼‚æ„GNNè®­ç»ƒåˆ†æå®Œæˆ!")
    logger.info(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒç»“æœ:")
    logger.info(f"   - æœ€ä½³è®­ç»ƒF1åˆ†æ•°: {best_train_f1:.4f}")
    logger.info(f"   - æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {training_history['train_accuracy'][-1]:.4f}")
    logger.info(f"   - æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_history['train_loss'][-1]:.4f}")
    logger.info(f"ğŸ”— å­¦ä¹ åˆ°çš„è¾¹ç±»å‹é‡è¦æ€§:")
    edge_names = ['AST', 'CFG', 'DFG', 'CDG']
    for name, weight in zip(edge_names, edge_importance):
        logger.info(f"   - {name}: {weight:.4f}")
    
    # æ‰¾å‡ºæ”¹è¿›æœ€å¤§å’Œæœ€å°çš„ç±»åˆ«
    improvements = {cwe: last_epoch[cwe] - first_epoch[cwe] for cwe in cwe_names}
    best_improved = max(improvements, key=improvements.get)
    worst_improved = min(improvements, key=improvements.get)
    
    logger.info(f"ğŸ† æ”¹è¿›æœ€å¤§çš„ç±»åˆ«: {best_improved} (+{improvements[best_improved]:.4f})")
    logger.info(f"ğŸ“‰ æ”¹è¿›æœ€å°çš„ç±»åˆ«: {worst_improved} ({improvements[worst_improved]:+.4f})")


if __name__ == "__main__":
    train_livable_enhanced_hetero_gnn()