#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨LIVABLEæ¨¡å‹è®­ç»ƒå¤šåˆ†ç±»æ¼æ´æ£€æµ‹
åŸºäºåŸå§‹LIVABLEæ¶æ„ï¼Œé€‚é…æˆ‘ä»¬çš„14ç±»CWEæ•°æ®
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from datetime import datetime
import logging
from pathlib import Path
import torch.nn.functional as F
from tqdm import tqdm
import copy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨å¤æ‚çš„DGLåŠŸèƒ½
try:
    import dgl
    from dgl import DGLGraph
    DGL_AVAILABLE = True
except ImportError:
    logger.warning("DGLä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    DGL_AVAILABLE = False

class MultiClassDataEntry:
    """æ•°æ®æ¡ç›®ç±»ï¼Œé€‚é…æˆ‘ä»¬çš„å¤šåˆ†ç±»æ•°æ®æ ¼å¼"""
    def __init__(self, features, edges, target, sequence, edge_types):
        self.num_nodes = len(features)
        self.target = target
        self.graph = DGLGraph()
        
        # å¤„ç†ç‰¹å¾ - ä»768ç»´é™åˆ°128ç»´ä»¥é€‚é…LIVABLE
        self.features = torch.FloatTensor(features)
        features_reduced = self.features[:, :128]  # å–å‰128ç»´
        
        self.graph.add_nodes(self.num_nodes, data={'features': features_reduced})
        self.seq = sequence
        self.edge_types = edge_types
        
        # æ·»åŠ è¾¹
        for s, edge_type, t in edges:
            if s < self.num_nodes and t < self.num_nodes:  # ç¡®ä¿èŠ‚ç‚¹ç´¢å¼•æœ‰æ•ˆ
                etype_number = self.get_edge_type_number(edge_type)
                self.graph.add_edge(s, t, data={'etype': torch.LongTensor([etype_number])})
    
    def get_edge_type_number(self, edge_type):
        """è·å–è¾¹ç±»å‹ç¼–å·"""
        if edge_type not in self.edge_types:
            self.edge_types[edge_type] = len(self.edge_types)
        return self.edge_types[edge_type]

class MultiClassDataset(Dataset):
    """å¤šåˆ†ç±»æ•°æ®é›†ç±»"""
    def __init__(self, data_path):
        self.data = []
        self.edge_types = {}
        self.max_etype = 0
        
        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®: {data_path}")
        with open(data_path, 'r') as f:
            raw_data = json.load(f)
        
        logger.info(f"ğŸ“Š å¤„ç† {len(raw_data)} ä¸ªæ ·æœ¬...")
        for item in tqdm(raw_data, desc="å¤„ç†æ•°æ®"):
            try:
                features = item['features']
                edges = item['structure']  # [[src, edge_type, dst], ...]
                target = item['label'][0][0]  # æå–æ ‡ç­¾
                sequence = item['sequence']
                
                data_entry = MultiClassDataEntry(features, edges, target, sequence, self.edge_types)
                self.data.append(data_entry)
                
            except Exception as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ ·æœ¬: {e}")
                continue
        
        self.max_etype = len(self.edge_types)
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} ä¸ªæ ·æœ¬")
        logger.info(f"ğŸ”— è¾¹ç±»å‹æ•°é‡: {self.max_etype}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

class LIVABLEMultiClassModel(nn.Module):
    """LIVABLEå¤šåˆ†ç±»æ¨¡å‹"""
    def __init__(self, input_dim=128, num_classes=14, max_edge_types=10):
        super(LIVABLEMultiClassModel, self).__init__()
        
        self.input_dim = input_dim
        self.num_classes = num_classes
        self.max_edge_types = max_edge_types
        
        # åŸºäºLIVABLEçš„DevignModelï¼Œä½†ä¿®æ”¹è¾“å‡ºç±»åˆ«æ•°
        self.devign_model = DevignModel(
            input_dim=input_dim,
            output_dim=128,  # éšè—å±‚ç»´åº¦
            max_edge_types=max_edge_types,
            num_steps=8
        )
        
        # ä¿®æ”¹æœ€åçš„åˆ†ç±»å±‚ä¸º14ç±»
        self.devign_model.MPL_layer = MLPReadout(256, num_classes)  # 256æ˜¯hidden_dim2
        self.devign_model.MPL_layer1 = MLPReadout(1024, num_classes)  # 2 * seq_hid
        
    def forward(self, batch_data, sequences):
        """å‰å‘ä¼ æ’­"""
        return self.devign_model(batch_data, sequences, cuda=True)

class LIVABLEBatchGraph:
    """LIVABLEå…¼å®¹çš„æ‰¹å›¾ç±»"""
    def __init__(self, graphs, sequences, targets):
        self.graphs = graphs
        self.sequences = sequences
        self.targets = targets
        self.batched_graph = dgl.batch(graphs)

    def get_network_inputs(self, cuda=False):
        """è·å–ç½‘ç»œè¾“å…¥ï¼Œå…¼å®¹LIVABLEæ ¼å¼"""
        # è·å–æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾
        features = self.batched_graph.ndata['features']
        if cuda:
            features = features.cuda()

        return self.batched_graph, features, None

    def de_batchify_graphs(self, features):
        """å°†æ‰¹ç‰¹å¾åˆ†è§£ä¸ºå•ä¸ªå›¾çš„ç‰¹å¾"""
        # ç®€åŒ–å®ç°ï¼šç›´æ¥è¿”å›ç‰¹å¾
        return features.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

    def en_batchify_graphs(self, features):
        """å°†å•ä¸ªå›¾ç‰¹å¾åˆå¹¶ä¸ºæ‰¹ç‰¹å¾"""
        # ç®€åŒ–å®ç°ï¼šç§»é™¤batchç»´åº¦
        return features.squeeze(0)

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    graphs = []
    sequences = []
    targets = []

    for item in batch:
        graphs.append(item.graph)
        sequences.append(item.seq)
        targets.append(item.target)

    # å¤„ç†åºåˆ— - å¡«å……åˆ°ç›¸åŒé•¿åº¦
    max_seq_len = max(len(seq) for seq in sequences) if sequences else 1
    padded_sequences = []
    for seq in sequences:
        if len(seq) < max_seq_len:
            seq = seq + [0] * (max_seq_len - len(seq))  # ç”¨0å¡«å……
        elif len(seq) > max_seq_len:
            seq = seq[:max_seq_len]  # æˆªæ–­
        padded_sequences.append(seq)

    # åˆ›å»ºLIVABLEå…¼å®¹çš„æ‰¹å›¾
    batch_graph = LIVABLEBatchGraph(graphs, padded_sequences, targets)

    return batch_graph, torch.FloatTensor(padded_sequences), torch.LongTensor(targets)

def setup_training_environment():
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("livable_training_results")
    output_dir.mkdir(exist_ok=True)
    
    return device, output_dir

def load_datasets():
    """åŠ è½½è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†"""
    train_dataset = MultiClassDataset('livable_multiclass_data/livable_train.json')
    valid_dataset = MultiClassDataset('livable_multiclass_data/livable_valid.json')
    test_dataset = MultiClassDataset('livable_multiclass_data/livable_test.json')
    
    return train_dataset, valid_dataset, test_dataset

def create_data_loaders(train_dataset, valid_dataset, test_dataset, batch_size=16):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    valid_loader = DataLoader(
        valid_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=0
    )
    
    return train_loader, valid_loader, test_loader

def evaluate_model(model, data_loader, device, criterion):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_graph, sequences, targets in tqdm(data_loader, desc="è¯„ä¼°ä¸­"):
            sequences = sequences.to(device)
            targets = targets.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(batch_graph, sequences)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item()
            
            # è·å–é¢„æµ‹ç»“æœ
            _, predicted = torch.max(outputs.data, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted', zero_division=0
    )
    
    avg_loss = total_loss / len(data_loader)
    
    return avg_loss, accuracy, precision, recall, f1, all_predictions, all_targets

def train_model(model, train_loader, valid_loader, device, num_epochs=50, learning_rate=0.0001):
    """è®­ç»ƒæ¨¡å‹"""
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)

    best_valid_f1 = 0
    best_model_state = None
    patience = 10
    patience_counter = 0

    training_history = {
        'train_loss': [],
        'train_acc': [],
        'valid_loss': [],
        'valid_acc': [],
        'valid_f1': []
    }

    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_graph, sequences, targets in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
            sequences = sequences.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(batch_graph, sequences)
            loss = criterion(outputs, targets)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += targets.size(0)
            train_correct += (predicted == targets).sum().item()

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_accuracy = train_correct / train_total
        avg_train_loss = train_loss / len(train_loader)

        # éªŒè¯é˜¶æ®µ
        valid_loss, valid_accuracy, valid_precision, valid_recall, valid_f1, _, _ = evaluate_model(
            model, valid_loader, device, criterion
        )

        # è®°å½•å†å²
        training_history['train_loss'].append(avg_train_loss)
        training_history['train_acc'].append(train_accuracy)
        training_history['valid_loss'].append(valid_loss)
        training_history['valid_acc'].append(valid_accuracy)
        training_history['valid_f1'].append(valid_f1)

        logger.info(f"Epoch {epoch+1}/{num_epochs}:")
        logger.info(f"  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}")
        logger.info(f"  éªŒè¯ - Loss: {valid_loss:.4f}, Acc: {valid_accuracy:.4f}, F1: {valid_f1:.4f}")

        # æ—©åœæ£€æŸ¥
        if valid_f1 > best_valid_f1:
            best_valid_f1 = valid_f1
            best_model_state = copy.deepcopy(model.state_dict())
            patience_counter = 0
            logger.info(f"  âœ… æ–°çš„æœ€ä½³F1åˆ†æ•°: {best_valid_f1:.4f}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"  â¹ï¸ æ—©åœè§¦å‘ï¼Œæœ€ä½³F1: {best_valid_f1:.4f}")
                break

    # æ¢å¤æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    return model, training_history, best_valid_f1

def save_results(model, training_history, test_results, output_dir):
    """ä¿å­˜è®­ç»ƒç»“æœ"""
    # ä¿å­˜æ¨¡å‹
    model_path = output_dir / "best_livable_model.pth"
    torch.save(model.state_dict(), model_path)
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = output_dir / "livable_training_history.json"
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2)
    logger.info(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")

    # ä¿å­˜æµ‹è¯•ç»“æœ
    results_path = output_dir / "livable_final_results.json"
    with open(results_path, 'w') as f:
        json.dump(test_results, f, indent=2)
    logger.info(f"ğŸ“ˆ æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¼€å§‹LIVABLEå¤šåˆ†ç±»æ¼æ´æ£€æµ‹è®­ç»ƒ")

    # è®¾ç½®ç¯å¢ƒ
    device, output_dir = setup_training_environment()

    # åŠ è½½æ•°æ®
    logger.info("ğŸ“¥ åŠ è½½æ•°æ®é›†...")
    train_dataset, valid_dataset, test_dataset = load_datasets()

    # è·å–è¾¹ç±»å‹æ•°é‡
    max_edge_types = max(train_dataset.max_etype, valid_dataset.max_etype, test_dataset.max_etype)
    logger.info(f"ğŸ”— æœ€å¤§è¾¹ç±»å‹æ•°: {max_edge_types}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, valid_loader, test_loader = create_data_loaders(
        train_dataset, valid_dataset, test_dataset, batch_size=16
    )

    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ—ï¸ åˆ›å»ºLIVABLEæ¨¡å‹...")
    model = LIVABLEMultiClassModel(
        input_dim=128,
        num_classes=14,
        max_edge_types=max_edge_types
    ).to(device)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")

    # è®­ç»ƒæ¨¡å‹
    model, training_history, best_valid_f1 = train_model(
        model, train_loader, valid_loader, device, num_epochs=50, learning_rate=0.0001
    )

    # æµ‹è¯•æ¨¡å‹
    logger.info("ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    criterion = nn.CrossEntropyLoss()
    test_loss, test_accuracy, test_precision, test_recall, test_f1, predictions, targets = evaluate_model(
        model, test_loader, device, criterion
    )

    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    with open('multiclass_label_mapping.json', 'r') as f:
        label_mapping = json.load(f)

    class_names = [label_mapping['label_to_cwe'][str(i)] for i in range(14)]
    classification_rep = classification_report(
        targets, predictions, target_names=class_names, output_dict=True, zero_division=0
    )

    # æ··æ·†çŸ©é˜µ
    conf_matrix = confusion_matrix(targets, predictions)

    # æ•´ç†æµ‹è¯•ç»“æœ
    test_results = {
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'test_precision': test_precision,
        'test_recall': test_recall,
        'test_f1': test_f1,
        'best_valid_f1': best_valid_f1,
        'classification_report': classification_rep,
        'confusion_matrix': conf_matrix.tolist(),
        'class_names': class_names,
        'model_info': {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'input_dim': 128,
            'num_classes': 14,
            'max_edge_types': max_edge_types
        }
    }

    # æ‰“å°ç»“æœ
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    logger.info(f"  å‡†ç¡®ç‡: {test_accuracy:.4f}")
    logger.info(f"  ç²¾ç¡®ç‡: {test_precision:.4f}")
    logger.info(f"  å¬å›ç‡: {test_recall:.4f}")
    logger.info(f"  F1åˆ†æ•°: {test_f1:.4f}")

    # ä¿å­˜ç»“æœ
    save_results(model, training_history, test_results, output_dir)

    logger.info("âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜å®Œæˆï¼")

if __name__ == "__main__":
    main()
